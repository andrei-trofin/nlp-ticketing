{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c024d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data handling libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "# Import functions from local scripts\n",
    "import sys\n",
    "sys.path.insert(1, './scripts/development')\n",
    "import scripts.development.preprocessing as pre\n",
    "import scripts.development.experiment as ex\n",
    "\n",
    "# Import modeling libraries\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Import metrics functions\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, average_precision_score\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, balanced_accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "# Import function libraries\n",
    "from functools import partial\n",
    "\n",
    "# Import balancing libraries\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\n",
    "from imblearn.under_sampling import ClusterCentroids, RandomUnderSampler, AllKNN\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "\n",
    "# Import I/O libraries\n",
    "from pickle import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b73f01a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>is_about_order_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- __EMAIL__ Hi , I have just ordered a pair of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am missing a pair of shoes from my order. Co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I didn'tget a my order - __EMAIL__</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hello, I ordered two __PRODUCTS_NAMES and one ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My shipment never was delivered. The tracking ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Description  is_about_order_status\n",
       "0  - __EMAIL__ Hi , I have just ordered a pair of...                      0\n",
       "1  I am missing a pair of shoes from my order. Co...                      0\n",
       "2                 I didn'tget a my order - __EMAIL__                      1\n",
       "3  Hello, I ordered two __PRODUCTS_NAMES and one ...                      0\n",
       "4  My shipment never was delivered. The tracking ...                      1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickets_df = pd.read_csv(\"../data/labeled_tickets.csv\", index_col=0)\n",
    "tickets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fa70c5d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>is_about_order_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- __EMAIL__ Hi , I have just ordered a pair of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  is_about_order_status\n",
       "0  - __EMAIL__ Hi , I have just ordered a pair of...                      0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename column name from Description to text.\n",
    "# I will keep this consistent throughout the functions\n",
    "tickets_df = tickets_df.rename(columns={'Description': 'text'})\n",
    "tickets_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9db5f1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that dtype is string\n",
    "tickets_df['text'] = tickets_df['text'].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2eba3644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate all text into English\n",
    "tickets_df['text'] = tickets_df['text'].apply(lambda x: pre.translate_to_en(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "147ed588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1249\n",
       "1     184\n",
       "Name: is_about_order_status, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how imbalanced the dataset is\n",
    "tickets_df[\"is_about_order_status\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11f6e31",
   "metadata": {},
   "source": [
    "### What is next?\n",
    "\n",
    "We can see that our dataset is very imbalanced. We will deal with that later. For now, I just want to extract some samples for testing and the remaining files for training. I will go with a 20% split.\n",
    "\n",
    "I will also define the preprocessing functions for the raw text in the python script files and test them accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cba06a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1146,) (287,) (1146,) (287,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tickets_df['text'], tickets_df[\"is_about_order_status\"],\n",
    "                                                    stratify=tickets_df[\"is_about_order_status\"],\n",
    "                                                    test_size=0.2)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d2172cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y_train and y_test to int pandas.Series\n",
    "y_train = y_train.astype('int8')\n",
    "y_test = y_test.astype('int8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aebc100",
   "metadata": {},
   "source": [
    "### Raw text processing\n",
    "\n",
    "1. I will want to check if a text is in a language different than English. If it is, I will translate it into English.\n",
    "2. I will measure text length and store it as a feature.\n",
    "3. I will measure average word length and store it as a feature.\n",
    "4. I will measure number of words and store it as a feature.\n",
    "4. I will use a library to reduce spelling errors.\n",
    "5. I will use a library to expand words like can't to can not.\n",
    "6. I will measure the numeric counts of each message (the count of number values present in each message)\n",
    "7. I will convert all text to lowercase\n",
    "8. I will get a set of all tokens in the texts (like __email__ and __company__ etc.) and I will create a column for each in which I will count their occurences. I will then remove these tokens from each text. The set of tokens will be stored in the script file to be used with the general processing function. It will be easy to later find new tokens and add them to the set.\n",
    "9. I will count number of emails in each message and remove them.\n",
    "10. I will count and remove any url from the text.\n",
    "11. I will measure the number of stopwords and store it as a feature.\n",
    "12. I will remove stopwords, single characters and special characters from text.\n",
    "13. I will stem the remaining words.\n",
    "\n",
    "All these steps will be wrapped into a function that preprocesses raw text. I will also create a function which uses the former function for a pandas series, so I can easily use it when testing ways to balance the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5e784dcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_ADDRESS_',\n",
       " '_COMPANY_',\n",
       " '_COMPANY__',\n",
       " '_COMPANY___',\n",
       " '_CREDIT_',\n",
       " '_DATE_',\n",
       " '_DATE__PRODUCT_',\n",
       " '_INVOICE_NUMBER_',\n",
       " '_ITEM_PHOTO_',\n",
       " '_LOCATION_',\n",
       " '_MONTH_',\n",
       " '_NAME_',\n",
       " '_NAME__ADRRESS_',\n",
       " '_NUMBER__',\n",
       " '_ORDER_NUMBBER_',\n",
       " '_ORDER_NUMBER_',\n",
       " '_OTHER_PI_',\n",
       " '_PHONE_',\n",
       " '_PRICE_',\n",
       " '_PRODUCTS_',\n",
       " '_PRODUCTS_NAMES_',\n",
       " '_PRODUCT_',\n",
       " '_PRODUCT_NAME_',\n",
       " '_TRACKING_NUMBER_',\n",
       " '_URL_',\n",
       " '__ADDRESS__',\n",
       " '__AMOUNT__',\n",
       " '__COMPANY_NAME__',\n",
       " '__COMPANY__',\n",
       " '__CREDIT_CARD__',\n",
       " '__DATE__',\n",
       " '__DATE____',\n",
       " '__DISCOUNT_CODE__',\n",
       " '__EMAIL__',\n",
       " '__INVOICE_NUMBER__',\n",
       " '__MAIN__',\n",
       " '__NAMES__',\n",
       " '__NAME__',\n",
       " '__NAME____',\n",
       " '__ORDER_NUMBER__',\n",
       " '__OTHER_PI__',\n",
       " '__PHONE__',\n",
       " '__PLACE__',\n",
       " '__PLACE____',\n",
       " '__PRODUCTS_NAMES__',\n",
       " '__PRODUCT_NAMES__',\n",
       " '__PRODUCT_NAME__',\n",
       " '__PRODUCT__NAMES__',\n",
       " '__PRODUCT__NAME__',\n",
       " '__REFERENCE_NUMBER__',\n",
       " '__TRACKING_NUMBER__',\n",
       " '__URL__'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Because ticket tokens are present in the raw text, we can extract here the token set and see if we spot anything wrong.\n",
    "token_set = pre.get_tokens(X_train)\n",
    "token_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97215260",
   "metadata": {},
   "source": [
    "We can spot some mistakes from our regex which we will eliminate from the set:\n",
    "\\_COMPANY___, \\_COMPANY__ \\_NAME__ADRRESS_, \\_NUMBER__, \\__DATE____, \\_DATE__PRODUCT_, \\__NAME____, \\__PLACE____\n",
    "\n",
    "In order to extract features from these tokens it is best to define a dictionary where the key is the column and the value is a list of tokens corresponding to the column. For each token in the list, we will increase the corresponding column value by 1.\n",
    "Example: column value is product_name and the token list is \\[\\_product_name_, \\_product_names_]. Then for each token of value \\_product_name_ or \\_product_names_, we will increase the product_name column value by 1.\n",
    "\n",
    "We will store the final dictionary into a file in order to be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "726a5015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove incorrect tokens\n",
    "token_set.difference({\"_COMPANY__\", \"_COMPANY___\", \"_NAME__ADRRESS_\", \"_NUMBER__\", \"__DATE____\",\n",
    "                      \"\\_DATE__PRODUCT_\", \"\\__NAME____\", \"\\__PLACE____\"})\n",
    "\n",
    "# Define dictionary\n",
    "token_dictionary = {\"address_count\": ['_ADDRESS_', '__ADDRESS__', '_LOCATION_', '__PLACE__'],\n",
    "                    \"company_name_count\": ['_COMPANNY_', '_COMPANY_', '__COMPANY_NAME__', '__COMPANY__'],\n",
    "                    \"credit_card_count\": ['_CREDIT_', '__CREDIT_CARD__'],\n",
    "                    \"date_count\": ['_DATE_', '_MONTH_', '__DATE__'],\n",
    "                    \"invoice_count\": ['_INVOICE_NUMBER_', '__INVOICE_NUMBER__'],\n",
    "                    \"photo_count\": ['_ITEM_PHOTO_'],\n",
    "                    \"name_count\": ['_NAME_', '__NAMES__', '__NAME__'],\n",
    "                    \"order_count\": ['_ORDER_NUMBBER_', '_ORDER_NUMBER_', '__ORDER_NUMBER__'],\n",
    "                    \"other_pi_count\": ['_OTHER_PI_', '__OTHER_PI__'],\n",
    "                    \"phone_count\": ['_PHONE_', '__PHONE__'],\n",
    "                    \"price_count\": ['_PRICE_', '__AMOUNT__'],\n",
    "                    \"product_count\": ['_PRODUCT_', '_PRODUCT_NAME_', '__PRODUCTS_NAMES__', '__PRODUCT_NAMES__', \n",
    "                                      '__PRODUCT_NAME__', '__PRODUCT__NAMES__', '__PRODUCT__NAME__'],\n",
    "                    \"tracking_number_count\": ['_TRACKING_NUMBER_', '__TRACKING_NUMBER__'],\n",
    "                    \"url_count\": ['_URL_', '__URL__'],\n",
    "                    \"discount_code_count\": ['__DISCOUNT_CODE__'],\n",
    "                    \"email_count\": ['__EMAIL__'],\n",
    "                    \"reference_number_count\": ['__REFERENCE_NUMBER__']\n",
    "                   }\n",
    "\n",
    "# Save token dictionary to file\n",
    "with open(\"../data/token_dictionary.json\", \"w\") as file:\n",
    "    json.dump(token_dictionary, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88adab6d",
   "metadata": {},
   "source": [
    "## Getting a first result\n",
    "\n",
    "We can now use the preprocessing functions defined in the python scripts to get a first result on some classifiers.\n",
    "I will use the tf-idf vectorizer from sklearn to achieve trainable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19fd8e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_tfidf_matrices(token_dictionary, token_list, train, test, with_manual_features=True):\n",
    "    \"\"\"\n",
    "    Function which takes train and test dataframes containing text, preprocesses them and converts them into sparse\n",
    "    matrices through a tf-idf vectorizer\n",
    "    :param token_dictionary: dictionary of tokens, where the key is the general meaning of the token and what we will\n",
    "    use for column naming and the value is a list of actual values found in the text.\n",
    "    :param token_list: The list of all token values.\n",
    "    :param train: The train dataset. It must be a pandas.Series containing rows of text.\n",
    "    :param test: The test dataset. It must be a pandas.Series containing rows of text.\n",
    "    :param with_manual_features: Flag. If set to False, do not append manual features to the resulting DataFrame.\n",
    "    Append them if set to True. Default is True.\n",
    "    :return: two sparse matrices, the first one being the representation of the train dataset, while the second is the\n",
    "    representation of the test dataset\n",
    "    \"\"\"\n",
    "    p_train = pre.preprocess_text_series(train, token_dictionary, token_list, with_manual_features)\n",
    "    m_feats_train = p_train.drop(columns='text')\n",
    "    text_train = p_train['text']\n",
    "    \n",
    "    p_test = pre.preprocess_text_series(test, token_dictionary, token_list, with_manual_features)\n",
    "    m_feats_test = p_test.drop(columns='text')\n",
    "    text_test = p_test['text']\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_train = tfidf_vectorizer.fit_transform(text_train)\n",
    "    tfidf_test = tfidf_vectorizer.transform(text_test)\n",
    "    \n",
    "    if with_manual_features:\n",
    "        scaler = MinMaxScaler()\n",
    "        scaled_m_feats_train = pd.DataFrame(scaler.fit_transform(m_feats_train), index = m_feats_train.index.values)\n",
    "        scaled_m_feats_test = pd.DataFrame(scaler.transform(m_feats_test), index = m_feats_test.index.values)\n",
    "        tfidf_train = hstack((tfidf_train, csr_matrix(scaled_m_feats_train)))\n",
    "        tfidf_test = hstack((tfidf_test, csr_matrix(scaled_m_feats_test)))\n",
    "    \n",
    "    return tfidf_train, tfidf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8329c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use partial so we can only call this function with parameters that actually change\n",
    "obtain_tfidf = partial(obtain_tfidf_matrices, token_dictionary, token_set, X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "24d0d0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(train, test, model, model_name, target_train=y_train):\n",
    "    \"\"\"\n",
    "    Function which trains a model on a train test and builds a dataframe containing a list of metrics based on testing\n",
    "    the model on the test data\n",
    "    :param train: The train dataset. It must be a pandas.Series containing rows of text.\n",
    "    :param test: The test dataset. It must be a pandas.Series containing rows of text.\n",
    "    :param model: The scikit-learn model that will be trained on the train data.\n",
    "    :param model_name: A string representing the name of the model to be trained and evaluated.\n",
    "    :param with_manual_features: Flag. If set to False, do not append manual features to the resulting DataFrame.\n",
    "    Append them if set to True. Default is True.\n",
    "    :return: a list of 6 items representing: \n",
    "    [model name, accuracy score, balanced accuracy score, f1 score of the negative class, f1 score of the positive class, \n",
    "    precision score of the positive class, roc-auc score of the model, precision-recall-auc score of the model]\n",
    "    \"\"\"\n",
    "    model.fit(train, target_train)\n",
    "    predicted = model.predict(test)\n",
    "\n",
    "    acc = accuracy_score(y_test, predicted)\n",
    "    balanced_acc = balanced_accuracy_score(y_test, predicted)\n",
    "    f1_sc = f1_score(y_test, predicted, average=None)\n",
    "    roc_auc_sc = roc_auc_score(y_test, model.predict_proba(test)[:, 1])\n",
    "    ap_auc = average_precision_score(y_test, model.predict_proba(test)[:, 1])\n",
    "    precision_of_positive = precision_score(y_test, predicted, zero_division=0)\n",
    "    return [model_name, acc, balanced_acc, f1_sc[0], f1_sc[1], precision_of_positive, roc_auc_sc, ap_auc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1dbb2440",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [MultinomialNB(), SVC(random_state=42, class_weight='balanced', probability=True, kernel='linear'),\n",
    "          RandomForestClassifier(n_estimators=150, random_state=42), LogisticRegression(random_state=42), \n",
    "          SGDClassifier(loss='log_loss', random_state=42)]\n",
    "model_names = ['Naive Bayes', 'Linear SVC', 'RandomForestClassifier', ' Logistic Regression', 'SGD Classifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "314605e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_all_models(train, test, target_train=y_train):\n",
    "    \"\"\"\n",
    "    Function which trains a model on a train test and builds a dataframe containing a list of metrics based on testing\n",
    "    the model on the test data\n",
    "    :param train: The train dataset. It must be a pandas.Series containing rows of text.\n",
    "    :param test: The test dataset. It must be a pandas.Series containing rows of text.\n",
    "    :param target_train: An iterable or pd.Series representing the target values of the train set. \n",
    "    This parameter is needed when resampling. Default: initial defined y_train from train-test splitting.\n",
    "    :return: a pd.Dataframe having scores defined at the function evaluate_model for all the models defined at models\n",
    "    and their names defined at model_names.\n",
    "    \"\"\"\n",
    "    scores = pd.DataFrame(\n",
    "        columns=[\"model_name\", \"accuracy\", \"balanced_accuracy\", \"f1_score_negative\",\n",
    "                 \"f1_score_positive\", \"precision_positive\", \"roc_auc_score\", \"ap_auc_score\"])\n",
    "    for model, model_name in list(zip(models, model_names)):\n",
    "        row = evaluate_model(train, test, model, model_name, target_train)\n",
    "        scores.loc[len(scores)] = row\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49d06d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list of sampling methods\n",
    "sampling_method_names = ['oversampling_SMOTE', 'oversampling_ADASYN', 'oversampling_random',\n",
    "                     'undersampling_Cluster_Centroids', 'undersampling_All_KNN', 'undersampling_random',\n",
    "                     'mixed_SMOTEEN', 'mixed_SMOTETomek']\n",
    "sampling_methods = [SMOTE(random_state=42), ADASYN(random_state=42), RandomOverSampler(random_state=42, sampling_strategy='minority'),\n",
    "                 ClusterCentroids(random_state=42), AllKNN(), RandomUnderSampler(sampling_strategy='majority' ,random_state=42),\n",
    "                 SMOTEENN(random_state=42), SMOTETomek(random_state=42)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2754efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_balancing(sampling_names, samplings, with_manual_features=True):\n",
    "    \"\"\"\n",
    "    Function which returns a DataFrame containing the scores for all of the models on all of the sampling methods.\n",
    "    :param sampling_names: A list containing the names of the sampling methods.\n",
    "    :param samplings: A list containing sampling instances, in the same order as sampling_names so that \n",
    "    they correspond to each other\n",
    "    :param with_manual_features: Whether to add manually defined features to the extracted tf-idf data extracted \n",
    "    at preprocessing phase.\n",
    "    \"\"\"\n",
    "    train, test = obtain_tfidf(with_manual_features)\n",
    "    results_df = evaluate_on_all_models(train, test)\n",
    "    row_length_per_loop = results_df.shape[0]\n",
    "    sampling_name_list = ['None'] * row_length_per_loop\n",
    "    \n",
    "    for sampling_name, sampling in list(zip(sampling_names, samplings)):\n",
    "        X_resampled, y_resampled = sampling.fit_resample(train, y_train)\n",
    "        temp_results_df = evaluate_on_all_models(X_resampled, test, y_resampled)\n",
    "        sampling_name_list.extend([sampling_name] * row_length_per_loop)\n",
    "        results_df = pd.concat([results_df, temp_results_df], ignore_index=True)\n",
    "    \n",
    "    return pd.concat([results_df, pd.Series(sampling_name_list, name='sampling_method')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "380f37be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 32 s\n",
      "Wall time: 29.2 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>balanced_accuracy</th>\n",
       "      <th>f1_score_negative</th>\n",
       "      <th>f1_score_positive</th>\n",
       "      <th>precision_positive</th>\n",
       "      <th>roc_auc_score</th>\n",
       "      <th>ap_auc_score</th>\n",
       "      <th>sampling_method</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>SGD Classifier</td>\n",
       "      <td>0.909408</td>\n",
       "      <td>0.913459</td>\n",
       "      <td>0.945833</td>\n",
       "      <td>0.723404</td>\n",
       "      <td>0.596491</td>\n",
       "      <td>0.968324</td>\n",
       "      <td>0.853625</td>\n",
       "      <td>undersampling_Cluster_Centroids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>0.919861</td>\n",
       "      <td>0.896432</td>\n",
       "      <td>0.952772</td>\n",
       "      <td>0.735632</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.971243</td>\n",
       "      <td>0.849615</td>\n",
       "      <td>undersampling_Cluster_Centroids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SGD Classifier</td>\n",
       "      <td>0.919861</td>\n",
       "      <td>0.723730</td>\n",
       "      <td>0.955513</td>\n",
       "      <td>0.596491</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.966811</td>\n",
       "      <td>0.824016</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SGD Classifier</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.842865</td>\n",
       "      <td>0.957916</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.963459</td>\n",
       "      <td>0.822563</td>\n",
       "      <td>oversampling_SMOTE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>SGD Classifier</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.842865</td>\n",
       "      <td>0.957916</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.963459</td>\n",
       "      <td>0.822563</td>\n",
       "      <td>mixed_SMOTETomek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SGD Classifier</td>\n",
       "      <td>0.933798</td>\n",
       "      <td>0.846865</td>\n",
       "      <td>0.962076</td>\n",
       "      <td>0.739726</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.964973</td>\n",
       "      <td>0.822131</td>\n",
       "      <td>oversampling_ADASYN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.897946</td>\n",
       "      <td>0.941667</td>\n",
       "      <td>0.702128</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.971676</td>\n",
       "      <td>0.821959</td>\n",
       "      <td>undersampling_Cluster_Centroids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.923345</td>\n",
       "      <td>0.760270</td>\n",
       "      <td>0.957031</td>\n",
       "      <td>0.645161</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.966054</td>\n",
       "      <td>0.818000</td>\n",
       "      <td>oversampling_SMOTE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.923345</td>\n",
       "      <td>0.760270</td>\n",
       "      <td>0.957031</td>\n",
       "      <td>0.645161</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.966054</td>\n",
       "      <td>0.818000</td>\n",
       "      <td>mixed_SMOTETomek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.933798</td>\n",
       "      <td>0.881405</td>\n",
       "      <td>0.961616</td>\n",
       "      <td>0.759494</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.965297</td>\n",
       "      <td>0.816872</td>\n",
       "      <td>oversampling_ADASYN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.937282</td>\n",
       "      <td>0.871892</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>0.743590</td>\n",
       "      <td>0.967027</td>\n",
       "      <td>0.816840</td>\n",
       "      <td>oversampling_SMOTE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.937282</td>\n",
       "      <td>0.871892</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>0.743590</td>\n",
       "      <td>0.967027</td>\n",
       "      <td>0.816840</td>\n",
       "      <td>mixed_SMOTETomek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.930314</td>\n",
       "      <td>0.879405</td>\n",
       "      <td>0.959514</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.697674</td>\n",
       "      <td>0.968649</td>\n",
       "      <td>0.815116</td>\n",
       "      <td>oversampling_random</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.923345</td>\n",
       "      <td>0.737243</td>\n",
       "      <td>0.957364</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.969568</td>\n",
       "      <td>0.813365</td>\n",
       "      <td>oversampling_random</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>SGD Classifier</td>\n",
       "      <td>0.930314</td>\n",
       "      <td>0.867892</td>\n",
       "      <td>0.959677</td>\n",
       "      <td>0.743590</td>\n",
       "      <td>0.707317</td>\n",
       "      <td>0.965622</td>\n",
       "      <td>0.813220</td>\n",
       "      <td>oversampling_random</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.831351</td>\n",
       "      <td>0.958084</td>\n",
       "      <td>0.712329</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.963027</td>\n",
       "      <td>0.811948</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>SGD Classifier</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.888919</td>\n",
       "      <td>0.957230</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.673913</td>\n",
       "      <td>0.969189</td>\n",
       "      <td>0.800043</td>\n",
       "      <td>undersampling_All_KNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>0.916376</td>\n",
       "      <td>0.917459</td>\n",
       "      <td>0.950207</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.618182</td>\n",
       "      <td>0.966486</td>\n",
       "      <td>0.799954</td>\n",
       "      <td>undersampling_All_KNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.923345</td>\n",
       "      <td>0.932973</td>\n",
       "      <td>0.954357</td>\n",
       "      <td>0.760870</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.970486</td>\n",
       "      <td>0.791165</td>\n",
       "      <td>oversampling_random</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.895470</td>\n",
       "      <td>0.606108</td>\n",
       "      <td>0.943182</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.967243</td>\n",
       "      <td>0.790774</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>0.850174</td>\n",
       "      <td>0.902486</td>\n",
       "      <td>0.906318</td>\n",
       "      <td>0.626087</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.958919</td>\n",
       "      <td>0.788090</td>\n",
       "      <td>mixed_SMOTEEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>SGD Classifier</td>\n",
       "      <td>0.808362</td>\n",
       "      <td>0.878486</td>\n",
       "      <td>0.876957</td>\n",
       "      <td>0.566929</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.960108</td>\n",
       "      <td>0.785583</td>\n",
       "      <td>mixed_SMOTEEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.912892</td>\n",
       "      <td>0.903946</td>\n",
       "      <td>0.948240</td>\n",
       "      <td>0.725275</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.969297</td>\n",
       "      <td>0.784573</td>\n",
       "      <td>oversampling_SMOTE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.912892</td>\n",
       "      <td>0.903946</td>\n",
       "      <td>0.948240</td>\n",
       "      <td>0.725275</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.969297</td>\n",
       "      <td>0.784573</td>\n",
       "      <td>mixed_SMOTETomek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.895470</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>0.943396</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.963946</td>\n",
       "      <td>0.783926</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.916376</td>\n",
       "      <td>0.744757</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.612903</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.963622</td>\n",
       "      <td>0.782154</td>\n",
       "      <td>oversampling_ADASYN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.916376</td>\n",
       "      <td>0.905946</td>\n",
       "      <td>0.950413</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.622642</td>\n",
       "      <td>0.965514</td>\n",
       "      <td>0.780171</td>\n",
       "      <td>undersampling_Cluster_Centroids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>0.930314</td>\n",
       "      <td>0.821838</td>\n",
       "      <td>0.960317</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.951243</td>\n",
       "      <td>0.778861</td>\n",
       "      <td>oversampling_SMOTE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>0.930314</td>\n",
       "      <td>0.821838</td>\n",
       "      <td>0.960317</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.951243</td>\n",
       "      <td>0.778861</td>\n",
       "      <td>mixed_SMOTETomek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.819838</td>\n",
       "      <td>0.958250</td>\n",
       "      <td>0.704225</td>\n",
       "      <td>0.735294</td>\n",
       "      <td>0.949730</td>\n",
       "      <td>0.777169</td>\n",
       "      <td>oversampling_ADASYN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.819838</td>\n",
       "      <td>0.958250</td>\n",
       "      <td>0.704225</td>\n",
       "      <td>0.735294</td>\n",
       "      <td>0.948432</td>\n",
       "      <td>0.776929</td>\n",
       "      <td>oversampling_random</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.912892</td>\n",
       "      <td>0.926973</td>\n",
       "      <td>0.947808</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.603448</td>\n",
       "      <td>0.967135</td>\n",
       "      <td>0.772606</td>\n",
       "      <td>oversampling_ADASYN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.898955</td>\n",
       "      <td>0.907459</td>\n",
       "      <td>0.939203</td>\n",
       "      <td>0.701031</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.964541</td>\n",
       "      <td>0.771411</td>\n",
       "      <td>undersampling_Cluster_Centroids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.885017</td>\n",
       "      <td>0.934000</td>\n",
       "      <td>0.929336</td>\n",
       "      <td>0.691589</td>\n",
       "      <td>0.528571</td>\n",
       "      <td>0.959243</td>\n",
       "      <td>0.767606</td>\n",
       "      <td>undersampling_random</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.916376</td>\n",
       "      <td>0.779297</td>\n",
       "      <td>0.952569</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.709677</td>\n",
       "      <td>0.961135</td>\n",
       "      <td>0.761778</td>\n",
       "      <td>undersampling_All_KNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>0.881533</td>\n",
       "      <td>0.885946</td>\n",
       "      <td>0.928270</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.956216</td>\n",
       "      <td>0.759695</td>\n",
       "      <td>undersampling_random</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>SGD Classifier</td>\n",
       "      <td>0.885017</td>\n",
       "      <td>0.899459</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>0.673267</td>\n",
       "      <td>0.531250</td>\n",
       "      <td>0.953622</td>\n",
       "      <td>0.736481</td>\n",
       "      <td>undersampling_random</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.888502</td>\n",
       "      <td>0.590595</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.944865</td>\n",
       "      <td>0.731744</td>\n",
       "      <td>undersampling_All_KNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.867596</td>\n",
       "      <td>0.912486</td>\n",
       "      <td>0.918103</td>\n",
       "      <td>0.654545</td>\n",
       "      <td>0.493151</td>\n",
       "      <td>0.954811</td>\n",
       "      <td>0.731489</td>\n",
       "      <td>mixed_SMOTEEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.906486</td>\n",
       "      <td>0.911063</td>\n",
       "      <td>0.637168</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.959027</td>\n",
       "      <td>0.723377</td>\n",
       "      <td>undersampling_random</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.905923</td>\n",
       "      <td>0.727243</td>\n",
       "      <td>0.947162</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.956757</td>\n",
       "      <td>0.722314</td>\n",
       "      <td>undersampling_All_KNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.759582</td>\n",
       "      <td>0.862000</td>\n",
       "      <td>0.839907</td>\n",
       "      <td>0.517483</td>\n",
       "      <td>0.349057</td>\n",
       "      <td>0.947676</td>\n",
       "      <td>0.721066</td>\n",
       "      <td>mixed_SMOTEEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.871080</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.931099</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.903676</td>\n",
       "      <td>0.709392</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.442509</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.316239</td>\n",
       "      <td>0.187817</td>\n",
       "      <td>0.950486</td>\n",
       "      <td>0.691618</td>\n",
       "      <td>mixed_SMOTEEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.881533</td>\n",
       "      <td>0.851405</td>\n",
       "      <td>0.929167</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.947027</td>\n",
       "      <td>0.670409</td>\n",
       "      <td>undersampling_random</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model_name  accuracy  balanced_accuracy  f1_score_negative  \\\n",
       "24          SGD Classifier  0.909408           0.913459           0.945833   \n",
       "21              Linear SVC  0.919861           0.896432           0.952772   \n",
       "4           SGD Classifier  0.919861           0.723730           0.955513   \n",
       "9           SGD Classifier  0.926829           0.842865           0.957916   \n",
       "44          SGD Classifier  0.926829           0.842865           0.957916   \n",
       "14          SGD Classifier  0.933798           0.846865           0.962076   \n",
       "22  RandomForestClassifier  0.902439           0.897946           0.941667   \n",
       "7   RandomForestClassifier  0.923345           0.760270           0.957031   \n",
       "42  RandomForestClassifier  0.923345           0.760270           0.957031   \n",
       "13     Logistic Regression  0.933798           0.881405           0.961616   \n",
       "8      Logistic Regression  0.937282           0.871892           0.963855   \n",
       "43     Logistic Regression  0.937282           0.871892           0.963855   \n",
       "18     Logistic Regression  0.930314           0.879405           0.959514   \n",
       "17  RandomForestClassifier  0.923345           0.737243           0.957364   \n",
       "19          SGD Classifier  0.930314           0.867892           0.959677   \n",
       "1               Linear SVC  0.926829           0.831351           0.958084   \n",
       "29          SGD Classifier  0.926829           0.888919           0.957230   \n",
       "26              Linear SVC  0.916376           0.917459           0.950207   \n",
       "15             Naive Bayes  0.923345           0.932973           0.954357   \n",
       "3      Logistic Regression  0.895470           0.606108           0.943182   \n",
       "36              Linear SVC  0.850174           0.902486           0.906318   \n",
       "39          SGD Classifier  0.808362           0.878486           0.876957   \n",
       "5              Naive Bayes  0.912892           0.903946           0.948240   \n",
       "40             Naive Bayes  0.912892           0.903946           0.948240   \n",
       "2   RandomForestClassifier  0.895470           0.594595           0.943396   \n",
       "12  RandomForestClassifier  0.916376           0.744757           0.953125   \n",
       "23     Logistic Regression  0.916376           0.905946           0.950413   \n",
       "6               Linear SVC  0.930314           0.821838           0.960317   \n",
       "41              Linear SVC  0.930314           0.821838           0.960317   \n",
       "11              Linear SVC  0.926829           0.819838           0.958250   \n",
       "16              Linear SVC  0.926829           0.819838           0.958250   \n",
       "10             Naive Bayes  0.912892           0.926973           0.947808   \n",
       "20             Naive Bayes  0.898955           0.907459           0.939203   \n",
       "32  RandomForestClassifier  0.885017           0.934000           0.929336   \n",
       "27  RandomForestClassifier  0.916376           0.779297           0.952569   \n",
       "31              Linear SVC  0.881533           0.885946           0.928270   \n",
       "34          SGD Classifier  0.885017           0.899459           0.930233   \n",
       "25             Naive Bayes  0.888502           0.590595           0.939394   \n",
       "37  RandomForestClassifier  0.867596           0.912486           0.918103   \n",
       "30             Naive Bayes  0.857143           0.906486           0.911063   \n",
       "28     Logistic Regression  0.905923           0.727243           0.947162   \n",
       "38     Logistic Regression  0.759582           0.862000           0.839907   \n",
       "0              Naive Bayes  0.871080           0.500000           0.931099   \n",
       "35             Naive Bayes  0.442509           0.680000           0.529412   \n",
       "33     Logistic Regression  0.881533           0.851405           0.929167   \n",
       "\n",
       "    f1_score_positive  precision_positive  roc_auc_score  ap_auc_score  \\\n",
       "24           0.723404            0.596491       0.968324      0.853625   \n",
       "21           0.735632            0.640000       0.971243      0.849615   \n",
       "4            0.596491            0.850000       0.966811      0.824016   \n",
       "9            0.720000            0.710526       0.963459      0.822563   \n",
       "44           0.720000            0.710526       0.963459      0.822563   \n",
       "14           0.739726            0.750000       0.964973      0.822131   \n",
       "22           0.702128            0.578947       0.971676      0.821959   \n",
       "7            0.645161            0.800000       0.966054      0.818000   \n",
       "42           0.645161            0.800000       0.966054      0.818000   \n",
       "13           0.759494            0.714286       0.965297      0.816872   \n",
       "8            0.763158            0.743590       0.967027      0.816840   \n",
       "43           0.763158            0.743590       0.967027      0.816840   \n",
       "18           0.750000            0.697674       0.968649      0.815116   \n",
       "17           0.620690            0.857143       0.969568      0.813365   \n",
       "19           0.743590            0.707317       0.965622      0.813220   \n",
       "1            0.712329            0.722222       0.963027      0.811948   \n",
       "29           0.746988            0.673913       0.969189      0.800043   \n",
       "26           0.739130            0.618182       0.966486      0.799954   \n",
       "15           0.760870            0.636364       0.970486      0.791165   \n",
       "3            0.347826            0.888889       0.967243      0.790774   \n",
       "36           0.626087            0.461538       0.958919      0.788090   \n",
       "39           0.566929            0.400000       0.960108      0.785583   \n",
       "5            0.725275            0.611111       0.969297      0.784573   \n",
       "40           0.725275            0.611111       0.969297      0.784573   \n",
       "2            0.318182            1.000000       0.963946      0.783926   \n",
       "12           0.612903            0.760000       0.963622      0.782154   \n",
       "23           0.733333            0.622642       0.965514      0.780171   \n",
       "6            0.714286            0.757576       0.951243      0.778861   \n",
       "41           0.714286            0.757576       0.951243      0.778861   \n",
       "11           0.704225            0.735294       0.949730      0.777169   \n",
       "16           0.704225            0.735294       0.948432      0.776929   \n",
       "10           0.736842            0.603448       0.967135      0.772606   \n",
       "20           0.701031            0.566667       0.964541      0.771411   \n",
       "32           0.691589            0.528571       0.959243      0.767606   \n",
       "27           0.647059            0.709677       0.961135      0.761778   \n",
       "31           0.660000            0.523810       0.956216      0.759695   \n",
       "34           0.673267            0.531250       0.953622      0.736481   \n",
       "25           0.304348            0.777778       0.944865      0.731744   \n",
       "37           0.654545            0.493151       0.954811      0.731489   \n",
       "30           0.637168            0.473684       0.959027      0.723377   \n",
       "28           0.571429            0.692308       0.956757      0.722314   \n",
       "38           0.517483            0.349057       0.947676      0.721066   \n",
       "0            0.000000            0.000000       0.903676      0.709392   \n",
       "35           0.316239            0.187817       0.950486      0.691618   \n",
       "33           0.638298            0.526316       0.947027      0.670409   \n",
       "\n",
       "                    sampling_method  \n",
       "24  undersampling_Cluster_Centroids  \n",
       "21  undersampling_Cluster_Centroids  \n",
       "4                              None  \n",
       "9                oversampling_SMOTE  \n",
       "44                 mixed_SMOTETomek  \n",
       "14              oversampling_ADASYN  \n",
       "22  undersampling_Cluster_Centroids  \n",
       "7                oversampling_SMOTE  \n",
       "42                 mixed_SMOTETomek  \n",
       "13              oversampling_ADASYN  \n",
       "8                oversampling_SMOTE  \n",
       "43                 mixed_SMOTETomek  \n",
       "18              oversampling_random  \n",
       "17              oversampling_random  \n",
       "19              oversampling_random  \n",
       "1                              None  \n",
       "29            undersampling_All_KNN  \n",
       "26            undersampling_All_KNN  \n",
       "15              oversampling_random  \n",
       "3                              None  \n",
       "36                    mixed_SMOTEEN  \n",
       "39                    mixed_SMOTEEN  \n",
       "5                oversampling_SMOTE  \n",
       "40                 mixed_SMOTETomek  \n",
       "2                              None  \n",
       "12              oversampling_ADASYN  \n",
       "23  undersampling_Cluster_Centroids  \n",
       "6                oversampling_SMOTE  \n",
       "41                 mixed_SMOTETomek  \n",
       "11              oversampling_ADASYN  \n",
       "16              oversampling_random  \n",
       "10              oversampling_ADASYN  \n",
       "20  undersampling_Cluster_Centroids  \n",
       "32             undersampling_random  \n",
       "27            undersampling_All_KNN  \n",
       "31             undersampling_random  \n",
       "34             undersampling_random  \n",
       "25            undersampling_All_KNN  \n",
       "37                    mixed_SMOTEEN  \n",
       "30             undersampling_random  \n",
       "28            undersampling_All_KNN  \n",
       "38                    mixed_SMOTEEN  \n",
       "0                              None  \n",
       "35                    mixed_SMOTEEN  \n",
       "33             undersampling_random  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "with_manual_df = evaluate_balancing(sampling_method_names, sampling_methods, True)\n",
    "with_manual_df.sort_values(by=['ap_auc_score', 'f1_score_positive', 'precision_positive'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8751d14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "wo_manual_df = evaluate_balancing(sampling_method_names, sampling_methods, False)\n",
    "wo_manual_df.sort_values(by=['ap_auc_score', 'f1_score_positive', 'precision_positive'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1264e5da",
   "metadata": {},
   "source": [
    "## Evaluating the models\n",
    "\n",
    "I evaluated my models multiple times using train test randomization. Most of the models do not achieve a good f1 score for the imbalanced class, even after using undersampling, oversampling or a mix of the two.\n",
    "There was however a particular data split that got us an f1 score of the imbalanced data set of over 70% for most of the models. I will save that model and test it in another notebook to see if it works well.\n",
    "\n",
    "What I want to achieve with this model is to accurately classify a message as \"where-is-my-order\" with great precision. Therefore, if two models have similar f-scores for the positive target variable, I would rather choose the one with higher precision.\n",
    "\n",
    "Since the metrics were very different for all the random train test splits, it makes sense to try another for of balancing: translating training text into other languages and then back to English to artifically create more examples in the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b76def9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly, I want to save the scaler, vectorizer and model that had the best results, in order to test it on real data.\n",
    "def get_scaler_and_vectorizer(token_dictionary, token_list, train):\n",
    "    \"\"\"\n",
    "    Function which takes train and test dataframes containing text, preprocesses them and returns the scaler and vectorizer\n",
    "    used on the training set.\n",
    "    :param token_dictionary: dictionary of tokens, where the key is the general meaning of the token and what we will\n",
    "    use for column naming and the value is a list of actual values found in the text.\n",
    "    :param token_list: The list of all token values.\n",
    "    :param train: The train dataset. It must be a pandas.Series containing rows of text.\n",
    "    :return: a MinMaxScaler instance and a TfidfVectorizer instance, in this order\n",
    "    \"\"\"\n",
    "    p_train = pre.preprocess_text_series(train, token_dictionary, token_list)\n",
    "    m_feats_train = p_train.drop(columns='text')\n",
    "    text_train = p_train['text']\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(decode_error='ignore')\n",
    "    tfidf_vectorizer.fit(text_train)\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(m_feats_train)\n",
    "    \n",
    "    return scaler, tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43cc8816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain scaler and vectorizer\n",
    "scaler, tfidf_vectorizer = get_scaler_and_vectorizer(token_dictionary, token_set, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5bb9718e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save them to files\n",
    "with open(\"../data/modeling/scaler_1.pkl\", \"wb\") as scaler_file:\n",
    "    dump(scaler, scaler_file)\n",
    "    scaler_file.close()\n",
    "    \n",
    "with open(\"../data/modeling/tfidf_1.pkl\", \"wb\") as tfidf_file:\n",
    "    dump(tfidf_vectorizer, tfidf_file)\n",
    "    tfidf_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9ce28cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the best model to data and save it to file\n",
    "train, test = obtain_tfidf()\n",
    "\n",
    "X_resampled, y_resampled = ClusterCentroids(random_state=42).fit_resample(train, y_train)\n",
    "\n",
    "sgd = SGDClassifier(random_state=42, loss='log_loss')\n",
    "sgd.fit(X_resampled, y_resampled)\n",
    "\n",
    "with open(\"../data/modeling/sgd_1.pkl\", \"wb\") as model_file:\n",
    "    dump(sgd, model_file)\n",
    "    model_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c1498e",
   "metadata": {},
   "source": [
    "## Experiment\n",
    "\n",
    "What I would like to try is to artificially enhance my data set by translating the imbalanced data class into another language, then back to English.\n",
    "\n",
    "As we have seen above, we have 1249 negative class counts and 184 positive class counts. That means that the majority class is 6.8 times bigger. In order to counterbalance this, we need to generate 6 times the data that we have now for the minority class. Or, in simpler terms, we need to translate our messages in 6 different languages, then back to English.\n",
    "\n",
    "For this, I will create a function in a python script and call it in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "707db1de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98    Hi, \n",
       " \n",
       "\n",
       " I still haven't received my trainers ...\n",
       "Name: text, dtype: string"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the indexes for all the positive examples in the training set\n",
    "positive_indexes = y_train.loc[y_train == 1].index.values\n",
    "\n",
    "# Get the actual psoitive examples in the training set\n",
    "X_train_pos = X_train[positive_indexes].astype('string')\n",
    "X_train_pos.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "425a034e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 18.3 s\n",
      "Wall time: 3min 49s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    Hi,\\n \\n\\n I still haven't received my sneaker...\n",
       "1    Hi,\\n \\n\\n I still haven't received my trainer...\n",
       "2    Hi,\\n \\n\\n I still haven't received my trainer...\n",
       "3    Hi,\\n \\n\\n I still haven't received my sneaker...\n",
       "4    Hey,\\n \\n\\n I still haven't received my traine...\n",
       "dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Augment the data\n",
    "aug_X_train = ex.enhance_series(X_train_pos)\n",
    "aug_X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ec180084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "882"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create index to add the data correctly; Last index from the dataset is 1432, so we'll start at 1433 and end at 1443 + 882\n",
    "index_values = [x for x in range(1433, 1433+882)]\n",
    "len(index_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "73373618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1433    Hi,\\n \\n\\n I still haven't received my sneaker...\n",
       "1434    Hi,\\n \\n\\n I still haven't received my trainer...\n",
       "1435    Hi,\\n \\n\\n I still haven't received my trainer...\n",
       "1436    Hi,\\n \\n\\n I still haven't received my sneaker...\n",
       "1437    Hey,\\n \\n\\n I still haven't received my traine...\n",
       "dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index the augmented text series\n",
    "aug_X_train.index = pd.Index(index_values)\n",
    "aug_X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e22039b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1433    1\n",
       "1434    1\n",
       "1435    1\n",
       "1436    1\n",
       "1437    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create indexed target series\n",
    "aug_y_train = pd.Series([1]*len(index_values), index=index_values)\n",
    "aug_y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a23bedd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2028,) (2028,)\n"
     ]
    }
   ],
   "source": [
    "# Now mix them all together to form a final training df\n",
    "final_X_train = pd.concat([X_train, aug_X_train])\n",
    "final_y_train = pd.concat([y_train, aug_y_train])\n",
    "print(final_X_train.shape, final_y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1cd5ad29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1029\n",
       "0     999\n",
       "dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check value counts to see if it's balanced\n",
    "final_y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7032fc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the tfidf representation of train and test sets\n",
    "final_X_train_tfidf, final_X_test_tfidf = obtain_tfidf_matrices(token_dictionary, token_set, final_X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d66298c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>balanced_accuracy</th>\n",
       "      <th>f1_score_negative</th>\n",
       "      <th>f1_score_positive</th>\n",
       "      <th>precision_positive</th>\n",
       "      <th>roc_auc_score</th>\n",
       "      <th>ap_auc_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.846690</td>\n",
       "      <td>0.831405</td>\n",
       "      <td>0.906383</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.447761</td>\n",
       "      <td>0.910703</td>\n",
       "      <td>0.557154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>0.885017</td>\n",
       "      <td>0.830378</td>\n",
       "      <td>0.931959</td>\n",
       "      <td>0.629213</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.902378</td>\n",
       "      <td>0.650329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.895470</td>\n",
       "      <td>0.847892</td>\n",
       "      <td>0.938272</td>\n",
       "      <td>0.659091</td>\n",
       "      <td>0.568627</td>\n",
       "      <td>0.921730</td>\n",
       "      <td>0.631479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.871080</td>\n",
       "      <td>0.845405</td>\n",
       "      <td>0.922432</td>\n",
       "      <td>0.618557</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.907135</td>\n",
       "      <td>0.574426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SGD Classifier</td>\n",
       "      <td>0.885017</td>\n",
       "      <td>0.818865</td>\n",
       "      <td>0.932238</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.907243</td>\n",
       "      <td>0.614693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name  accuracy  balanced_accuracy  f1_score_negative  \\\n",
       "0             Naive Bayes  0.846690           0.831405           0.906383   \n",
       "1              Linear SVC  0.885017           0.830378           0.931959   \n",
       "2  RandomForestClassifier  0.895470           0.847892           0.938272   \n",
       "3     Logistic Regression  0.871080           0.845405           0.922432   \n",
       "4          SGD Classifier  0.885017           0.818865           0.932238   \n",
       "\n",
       "   f1_score_positive  precision_positive  roc_auc_score  ap_auc_score  \n",
       "0           0.576923            0.447761       0.910703      0.557154  \n",
       "1           0.629213            0.538462       0.902378      0.650329  \n",
       "2           0.659091            0.568627       0.921730      0.631479  \n",
       "3           0.618557            0.500000       0.907135      0.574426  \n",
       "4           0.620690            0.540000       0.907243      0.614693  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = evaluate_on_all_models(final_X_train_tfidf, final_X_test_tfidf, final_y_train)\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
