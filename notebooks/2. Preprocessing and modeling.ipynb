{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c024d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data handling libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "# Import functions from local scripts\n",
    "import sys\n",
    "sys.path.insert(1, './scripts/development')\n",
    "import scripts.development.preprocessing as pre\n",
    "import scripts.development.experiment as ex\n",
    "\n",
    "# Import modeling libraries\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate, RepeatedStratifiedKFold\n",
    "\n",
    "# Import metrics functions\n",
    "from sklearn.metrics import classification_report, f1_score, recall_score, average_precision_score\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, balanced_accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "# Import function libraries\n",
    "from functools import partial\n",
    "\n",
    "# Import balancing libraries\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\n",
    "from imblearn.under_sampling import ClusterCentroids, RandomUnderSampler, AllKNN\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "\n",
    "# Import I/O libraries\n",
    "from pickle import dump\n",
    "\n",
    "# Filter warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b73f01a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>is_about_order_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- __EMAIL__ Hi , I have just ordered a pair of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am missing a pair of shoes from my order. Co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I didn'tget a my order - __EMAIL__</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hello, I ordered two __PRODUCTS_NAMES and one ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My shipment never was delivered. The tracking ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Description  is_about_order_status\n",
       "0  - __EMAIL__ Hi , I have just ordered a pair of...                      0\n",
       "1  I am missing a pair of shoes from my order. Co...                      0\n",
       "2                 I didn'tget a my order - __EMAIL__                      1\n",
       "3  Hello, I ordered two __PRODUCTS_NAMES and one ...                      0\n",
       "4  My shipment never was delivered. The tracking ...                      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickets_df = pd.read_csv(\"../data/labeled_tickets.csv\", index_col=0)\n",
    "tickets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa70c5d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>is_about_order_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- __EMAIL__ Hi , I have just ordered a pair of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  is_about_order_status\n",
       "0  - __EMAIL__ Hi , I have just ordered a pair of...                      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename column name from Description to text.\n",
    "# I will keep this consistent throughout the functions\n",
    "tickets_df = tickets_df.rename(columns={'Description': 'text'})\n",
    "tickets_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9db5f1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that dtype is string\n",
    "tickets_df['text'] = tickets_df['text'].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2eba3644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate all text into English\n",
    "tickets_df['text'] = tickets_df['text'].apply(lambda x: pre.translate_to_en(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "147ed588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1249\n",
       "1     184\n",
       "Name: is_about_order_status, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how imbalanced the dataset is\n",
    "tickets_df[\"is_about_order_status\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11f6e31",
   "metadata": {},
   "source": [
    "### What is next?\n",
    "\n",
    "We can see that our dataset is very imbalanced. We will deal with that later. For now, I just want to extract some samples for testing and the remaining files for training. I will go with a 20% split.\n",
    "\n",
    "I will also define the preprocessing functions for the raw text in the python script files and test them accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cba06a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1146,) (287,) (1146,) (287,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tickets_df['text'], tickets_df[\"is_about_order_status\"],\n",
    "                                                    stratify=tickets_df[\"is_about_order_status\"],\n",
    "                                                    test_size=0.2)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2172cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y_train and y_test to int pandas.Series\n",
    "y_train = y_train.astype('int8')\n",
    "y_test = y_test.astype('int8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aebc100",
   "metadata": {},
   "source": [
    "### Raw text processing\n",
    "\n",
    "1. I will want to check if a text is in a language different than English. If it is, I will translate it into English.\n",
    "2. I will measure text length and store it as a feature.\n",
    "3. I will measure average word length and store it as a feature.\n",
    "4. I will measure number of words and store it as a feature.\n",
    "4. I will use a library to reduce spelling errors.\n",
    "5. I will use a library to expand words like can't to can not.\n",
    "6. I will measure the numeric counts of each message (the count of number values present in each message)\n",
    "7. I will convert all text to lowercase\n",
    "8. I will get a set of all tokens in the texts (like __email__ and __company__ etc.) and I will create a column for each in which I will count their occurences. I will then remove these tokens from each text. The set of tokens will be stored in the script file to be used with the general processing function. It will be easy to later find new tokens and add them to the set.\n",
    "9. I will count number of emails in each message and remove them.\n",
    "10. I will count and remove any url from the text.\n",
    "11. I will measure the number of stopwords and store it as a feature.\n",
    "12. I will remove stopwords, single characters and special characters from text.\n",
    "13. I will stem the remaining words.\n",
    "\n",
    "All these steps will be wrapped into a function that preprocesses raw text. I will also create a function which uses the former function for a pandas series, so I can easily use it when testing ways to balance the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e784dcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_ADDRESS_',\n",
       " '_COMPANNY_',\n",
       " '_COMPANY_',\n",
       " '_COMPANY__',\n",
       " '_COMPANY___',\n",
       " '_CREDIT_',\n",
       " '_DATE_',\n",
       " '_DATE__PRODUCT_',\n",
       " '_INVOICE_NUMBER_',\n",
       " '_ITEM_PHOTO_',\n",
       " '_LOCATION_',\n",
       " '_MONTH_',\n",
       " '_NAME_',\n",
       " '_NAME__ADRRESS_',\n",
       " '_ORDER_NUMBBER_',\n",
       " '_ORDER_NUMBER_',\n",
       " '_OTHER_PI_',\n",
       " '_PHONE_',\n",
       " '_PRICE_',\n",
       " '_PRODUCTS_',\n",
       " '_PRODUCTS_NAMES_',\n",
       " '_PRODUCT_',\n",
       " '_PRODUCT_NAME_',\n",
       " '_TRACKING_NUMBER_',\n",
       " '_URL_',\n",
       " '__ADDRESS__',\n",
       " '__AMOUNT__',\n",
       " '__COMPANY_NAME__',\n",
       " '__COMPANY__',\n",
       " '__CREDIT_CARD__',\n",
       " '__DATE__',\n",
       " '__DISCOUNT_CODE__',\n",
       " '__EMAIL__',\n",
       " '__INVOICE_NUMBER__',\n",
       " '__MAIN__',\n",
       " '__NAMES__',\n",
       " '__NAME__',\n",
       " '__NAME____',\n",
       " '__ORDER_NUMBER__',\n",
       " '__OTHER_PI__',\n",
       " '__PHONE__',\n",
       " '__PLACE__',\n",
       " '__PLACE____',\n",
       " '__PRODUCTS_NAMES__',\n",
       " '__PRODUCT_NAMES__',\n",
       " '__PRODUCT_NAME__',\n",
       " '__PRODUCT__NAMES__',\n",
       " '__PRODUCT__NAME__',\n",
       " '__REFERENCE_NUMBER__',\n",
       " '__TRACKING_NUMBER__',\n",
       " '__URL__'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Because ticket tokens are present in the raw text, we can extract here the token set and see if we spot anything wrong.\n",
    "token_set = pre.get_tokens(X_train)\n",
    "token_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97215260",
   "metadata": {},
   "source": [
    "We can spot some mistakes from our regex which we will eliminate from the set:\n",
    "\\_COMPANY___, \\_COMPANY__ \\_NAME__ADRRESS_, \\_NUMBER__, \\__DATE____, \\_DATE__PRODUCT_, \\__NAME____, \\__PLACE____\n",
    "\n",
    "In order to extract features from these tokens it is best to define a dictionary where the key is the column and the value is a list of tokens corresponding to the column. For each token in the list, we will increase the corresponding column value by 1.\n",
    "Example: column value is product_name and the token list is \\[\\_product_name_, \\_product_names_]. Then for each token of value \\_product_name_ or \\_product_names_, we will increase the product_name column value by 1.\n",
    "\n",
    "We will store the final dictionary into a file in order to be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21801856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_ADDRESS_',\n",
       " '_COMPANNY_',\n",
       " '_COMPANY_',\n",
       " '_CREDIT_',\n",
       " '_DATE_',\n",
       " '_DATE__PRODUCT_',\n",
       " '_INVOICE_NUMBER_',\n",
       " '_ITEM_PHOTO_',\n",
       " '_LOCATION_',\n",
       " '_MONTH_',\n",
       " '_NAME_',\n",
       " '_ORDER_NUMBBER_',\n",
       " '_ORDER_NUMBER_',\n",
       " '_OTHER_PI_',\n",
       " '_PHONE_',\n",
       " '_PRICE_',\n",
       " '_PRODUCTS_',\n",
       " '_PRODUCTS_NAMES_',\n",
       " '_PRODUCT_',\n",
       " '_PRODUCT_NAME_',\n",
       " '_TRACKING_NUMBER_',\n",
       " '_URL_',\n",
       " '__ADDRESS__',\n",
       " '__AMOUNT__',\n",
       " '__COMPANY_NAME__',\n",
       " '__COMPANY__',\n",
       " '__CREDIT_CARD__',\n",
       " '__DATE__',\n",
       " '__DISCOUNT_CODE__',\n",
       " '__EMAIL__',\n",
       " '__INVOICE_NUMBER__',\n",
       " '__MAIN__',\n",
       " '__NAMES__',\n",
       " '__NAME__',\n",
       " '__NAME____',\n",
       " '__ORDER_NUMBER__',\n",
       " '__OTHER_PI__',\n",
       " '__PHONE__',\n",
       " '__PLACE__',\n",
       " '__PLACE____',\n",
       " '__PRODUCTS_NAMES__',\n",
       " '__PRODUCT_NAMES__',\n",
       " '__PRODUCT_NAME__',\n",
       " '__PRODUCT__NAMES__',\n",
       " '__PRODUCT__NAME__',\n",
       " '__REFERENCE_NUMBER__',\n",
       " '__TRACKING_NUMBER__',\n",
       " '__URL__'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove incorrect tokens and check all the tokens visually\n",
    "token_set.difference({\"_COMPANY__\", \"_COMPANY___\", \"_NAME__ADRRESS_\", \"_NUMBER__\", \"__DATE____\",\n",
    "                      \"\\_DATE__PRODUCT_\", \"\\__NAME____\", \"\\__PLACE____\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "726a5015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dictionary\n",
    "token_dictionary = {\"address_count\": ['_ADDRESS_', '__ADDRESS__', '_LOCATION_', '__PLACE__'],\n",
    "                    \"company_name_count\": ['_COMPANNY_', '_COMPANY_', '__COMPANY_NAME__', '__COMPANY__'],\n",
    "                    \"credit_card_count\": ['_CREDIT_', '__CREDIT_CARD__'],\n",
    "                    \"date_count\": ['_DATE_', '_MONTH_', '__DATE__'],\n",
    "                    \"invoice_count\": ['_INVOICE_NUMBER_', '__INVOICE_NUMBER__'],\n",
    "                    \"photo_count\": ['_ITEM_PHOTO_'],\n",
    "                    \"name_count\": ['_NAME_', '__NAMES__', '__NAME__'],\n",
    "                    \"order_count\": ['_ORDER_NUMBBER_', '_ORDER_NUMBER_', '__ORDER_NUMBER__'],\n",
    "                    \"other_pi_count\": ['_OTHER_PI_', '__OTHER_PI__'],\n",
    "                    \"phone_count\": ['_PHONE_', '__PHONE__'],\n",
    "                    \"price_count\": ['_PRICE_', '__AMOUNT__'],\n",
    "                    \"product_count\": ['_PRODUCT_', '_PRODUCT_NAME_', '__PRODUCTS_NAMES__', '__PRODUCT_NAMES__', \n",
    "                                      '__PRODUCT_NAME__', '__PRODUCT__NAMES__', '__PRODUCT__NAME__'],\n",
    "                    \"tracking_number_count\": ['_TRACKING_NUMBER_', '__TRACKING_NUMBER__'],\n",
    "                    \"url_count\": ['_URL_', '__URL__'],\n",
    "                    \"discount_code_count\": ['__DISCOUNT_CODE__'],\n",
    "                    \"email_count\": ['__EMAIL__'],\n",
    "                    \"reference_number_count\": ['__REFERENCE_NUMBER__']\n",
    "                   }\n",
    "\n",
    "# However, because there are some tokens that are contained within other tokens, we found a way to use the dictionary above\n",
    "# such that we can replace the biggest strings first. And so we want to obtain from the dictionary above something like\n",
    "# {'_ADDRESS_': 'address_count', '__ADDRESS__': 'address_count', '_LOCATION_': 'address_count'} etc.\n",
    "# We do this by converting our dictionary in the following way\n",
    "new_token_dict = {}\n",
    "for key, val in token_dictionary.items():\n",
    "    for x in val:\n",
    "        # We now have each value in the lists as key. And the value at that key is the original value of the key.\n",
    "        # Or simply put: the group the item in the list is part of\n",
    "        new_token_dict.setdefault(x, key)\n",
    "\n",
    "# Save token dictionary to file\n",
    "with open(\"../data/token_dictionary.json\", \"w\") as file:\n",
    "    json.dump(new_token_dict, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88adab6d",
   "metadata": {},
   "source": [
    "## Getting a first result\n",
    "\n",
    "We can now use the preprocessing functions defined in the python scripts to get a first result on some classifiers.\n",
    "I will use the tf-idf vectorizer from sklearn to achieve trainable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b429fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_vectorizer(cleaned_text):\n",
    "    \"\"\"\n",
    "    Function which takes a dataframe containing preprocessed text, fits a tf-idf vectorizer through it \n",
    "    and returns the vectorizer.\n",
    "    :param cleaned_text: The preprocessed text on which we will use tf-idf.\n",
    "    :return: A scikit-learn.TfidfVectorizer instance that was fitted on the provided preprocessed text.\n",
    "    \"\"\"\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_vectorizer.fit(cleaned_text)\n",
    "    \n",
    "    return tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff0b0e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaler(manual_features):\n",
    "    \"\"\"\n",
    "    Function which takes a dataframe containing preprocessed text, fits a scaler through it \n",
    "    and returns the scaler.\n",
    "    :param manual_features: TA dataframe representing the manual features consisting of numeric values.\n",
    "    :return: A scikit-learn.MinMaxScaler instance that was fitted on the provided dataframe.\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(manual_features)\n",
    "    \n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bac50ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_m_feats(dataset, token_dictionary, with_manual_features):\n",
    "    \"\"\"\n",
    "    Function which takes a dataframe containing text, preprocesses it and returns two dataframes representing the\n",
    "    cleaned text dataframe and the extracted manual features.\n",
    "    :param dataset: The text dataset. It must be a pandas.Series containing rows of text.\n",
    "    :param token_dictionary: dictionary of tokens, where the key is the general meaning of the token and what we will\n",
    "    use for column naming and the value is a list of actual values found in the text.\n",
    "    :param with_manual_features: Flag. If set to True, return the extracted manual features as dataframe. \n",
    "    If set to False, return a None value instead.\n",
    "    :return: Two pandas.DataFrame s, the first representing the cleaned text and the second representing the manual features.\n",
    "    \"\"\"\n",
    "    p_dataset = pre.preprocess_text_series(dataset, token_dictionary, with_manual_features)\n",
    "    m_feats_dataset = p_dataset.drop(columns='text')\n",
    "    text_dataset = p_dataset['text']\n",
    "    \n",
    "    if with_manual_features:\n",
    "        return text_dataset, m_feats_dataset\n",
    "    else:\n",
    "        return text_dataset, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19fd8e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_tfidf_matrix(token_dictionary, train, with_manual_features=True):\n",
    "    \"\"\"\n",
    "    Function which takes a train dataframe containing text, preprocesses it and converts it into a sparse\n",
    "    matrix through a tf-idf vectorizer.\n",
    "    :param token_dictionary: dictionary of tokens, where the key is the general meaning of the token and what we will\n",
    "    use for column naming and the value is a list of actual values found in the text.\n",
    "    :param train: The train dataset. It must be a pandas.Series containing rows of text.\n",
    "    :param with_manual_features: Flag. If set to False, do not append manual features to the resulting DataFrame.\n",
    "    Append them if set to True. Default is True.\n",
    "    :return: A sparse matrix, being the representation of the train dataset after tf-idf vectorization.\n",
    "    \"\"\"\n",
    "    text_train, m_feats_train = get_text_m_feats(train, token_dictionary, with_manual_features)\n",
    "    \n",
    "    tfidf_vectorizer = get_tfidf_vectorizer(text_train)\n",
    "    tfidf_train = tfidf_vectorizer.transform(text_train)\n",
    "    \n",
    "    if with_manual_features:\n",
    "        scaler = get_scaler(m_feats_train)\n",
    "        scaled_m_feats_train = pd.DataFrame(scaler.transform(m_feats_train), index = m_feats_train.index.values)\n",
    "        tfidf_train = hstack((tfidf_train, csr_matrix(scaled_m_feats_train)))\n",
    "    \n",
    "    return tfidf_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33811624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_tfidf_matrix(token_dictionary, tfidf_vectorizer, scaler, test, with_manual_features=True):\n",
    "    \"\"\"\n",
    "    Function which takes a test dataframe containing text, preprocesses it and converts it into a sparse\n",
    "    matrix through a tf-idf vectorizer.\n",
    "    :param token_dictionary: dictionary of tokens, where the key is the general meaning of the token and what we will\n",
    "    use for column naming and the value is a list of actual values found in the text.\n",
    "    :param tfidf_vectorizer: A scikit-learn.TfidfVectorizer that is already fit on data.\n",
    "    :param scaler: A scikit-learn.MinMaxScaler that is already fit on data.\n",
    "    :param test: The test dataset. It must be a pandas.Series containing rows of text.\n",
    "    :param with_manual_features: Flag. If set to False, do not append manual features to the resulting DataFrame.\n",
    "    Append them if set to True. Default is True.\n",
    "    :return: A sparse matrix, being the representation of the test dataset after tf-idf vectorization.\n",
    "    \"\"\"\n",
    "    text_test, m_feats_test = get_text_m_feats(test, token_dictionary, with_manual_features)\n",
    "    \n",
    "    tfidf_test = tfidf_vectorizer.transform(text_test)\n",
    "    \n",
    "    if with_manual_features:\n",
    "        scaled_m_feats_test = pd.DataFrame(scaler.transform(m_feats_test), index = m_feats_test.index.values)\n",
    "        tfidf_test = hstack((tfidf_test, csr_matrix(scaled_m_feats_test)))\n",
    "    \n",
    "    return tfidf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8329c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use partial so we can only call this function with parameters that actually change\n",
    "obtain_tfidf = partial(obtain_tfidf_matrix, new_token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ef2c488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(train, model, model_name, target_train=y_train):\n",
    "    \"\"\"\n",
    "    Function which trains a model on a train set and builds a dataframe containing a list of metrics based on testing\n",
    "    the model through cross validation.\n",
    "    :param train: The train dataset. It must be a pandas.Series containing rows of text.\n",
    "    :param model: The scikit-learn model that will be trained on the train data.\n",
    "    :param model_name: A string representing the name of the model to be trained and evaluated.\n",
    "    :param with_manual_features: Flag. If set to False, do not append manual features to the resulting DataFrame.\n",
    "    Append them if set to True. Default is True.\n",
    "    :return: a dataframe containing values for the following columns: \n",
    "    [model_name, test_average_precision, train_average_precision, test_f1, train_f1, \n",
    "    test_recall, train_recall, test_accuracy, train_accuracy]\n",
    "    \"\"\"\n",
    "    # Get the mean of every metric on a RepeatedStratifiedKFold that does 5 random splits, 5 times, meaning that\n",
    "    # we fit the model 25 times and average the result\n",
    "    results = cross_validate(model, train, target_train, \n",
    "                   scoring=['average_precision', 'f1', 'recall', 'accuracy'], return_estimator=True,\n",
    "                             return_train_score=True, cv=RepeatedStratifiedKFold(n_repeats=5))\n",
    "    # Model the results such that the object will be easy to work with\n",
    "    results = pd.DataFrame(results)\n",
    "    # We can drop the estimator because we have the model_name provided\n",
    "    results = results.drop(columns=['fit_time', 'score_time', 'estimator']).mean(numeric_only=True)\n",
    "    results = results.rename(model_name)\n",
    "    \n",
    "    # The mean function gives us a pandas.Series having the metric as indexes. We need the metrics as columns.\n",
    "    # We also take the index (model_name) and add it as a column. This will make it easy for us to\n",
    "    # see the results for each model type\n",
    "    return pd.DataFrame(results).T.reset_index().rename(columns={'index': 'model_name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1dbb2440",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [MultinomialNB(), SVC(random_state=42, class_weight='balanced', probability=True, kernel='linear'),\n",
    "          RandomForestClassifier(n_estimators=150, random_state=42), LogisticRegression(random_state=42), \n",
    "          SGDClassifier(loss='log_loss', random_state=42)]\n",
    "model_names = ['Naive Bayes', 'Linear SVC', 'RandomForestClassifier', ' Logistic Regression', 'SGD Classifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "de99fbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_all_models(train, model_names, models, target_train=y_train):\n",
    "    \"\"\"\n",
    "    Function which trains a model on a train test and builds a dataframe containing a list of metrics based on testing\n",
    "    the model through cross validation.\n",
    "    :param train: The train dataset. It must be a pandas.Series containing rows of text.\n",
    "    :param test: The test dataset. It must be a pandas.Series containing rows of text.\n",
    "    :param model_names: A string list representing model names.\n",
    "    :param models: A list of sci-kit learn model objects. Must be in same order and of same length as model_names.\n",
    "    :param target_train: An iterable or pd.Series representing the target values of the train set. \n",
    "    This parameter is needed when resampling. Default: initial defined y_train from train-test splitting.\n",
    "    :return: a pd.Dataframe having scores defined at the function evaluate_model for all the models defined at models\n",
    "    and their names defined at model_names.\n",
    "    \"\"\"\n",
    "    scores = pd.DataFrame(\n",
    "        columns=[\"model_name\", \"test_average_precision\", \"train_average_precision\", \"test_f1\",\n",
    "                 \"train_f1\", \"test_recall\", \"train_recall\", \"test_accuracy\", \"train_accuracy\"])\n",
    "    for model, model_name in list(zip(models, model_names)):\n",
    "        row = evaluate_model(train, model, model_name, target_train)\n",
    "        scores = pd.concat([scores, row])\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49d06d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list of sampling methods\n",
    "sampling_method_names = ['oversampling_SMOTE', 'oversampling_ADASYN', 'oversampling_random',\n",
    "                     'undersampling_Cluster_Centroids', 'undersampling_All_KNN', 'undersampling_random',\n",
    "                     'mixed_SMOTEEN', 'mixed_SMOTETomek']\n",
    "sampling_methods = [SMOTE(random_state=42), ADASYN(random_state=42), RandomOverSampler(random_state=42, sampling_strategy='minority'),\n",
    "                 ClusterCentroids(random_state=42), AllKNN(), RandomUnderSampler(sampling_strategy='majority' ,random_state=42),\n",
    "                 SMOTEENN(random_state=42), SMOTETomek(random_state=42)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2754efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_balancing(sampling_names, samplings, model_names, models, train, with_manual_features=True):\n",
    "    \"\"\"\n",
    "    Function which returns a DataFrame containing the scores for all of the models on all of the sampling methods.\n",
    "    :param sampling_names: A list containing the names of the sampling methods.\n",
    "    :param samplings: A list containing sampling instances, in the same order as sampling_names so that \n",
    "    they correspond to each other\n",
    "    :param model_names: A string list representing model names.\n",
    "    :param models: A list of sci-kit learn model objects. Must be in same order and of same length as model_names.\n",
    "    :param train: The train dataset. It must be a pandas.Series containing rows of text.\n",
    "    :param with_manual_features: Whether to add manually defined features to the extracted tf-idf data extracted \n",
    "    at preprocessing phase.\n",
    "    \"\"\"\n",
    "    train = obtain_tfidf(train, with_manual_features)\n",
    "    results_df = evaluate_on_all_models(train, model_names, models)\n",
    "    row_length_per_loop = results_df.shape[0]\n",
    "    sampling_name_list = ['None'] * row_length_per_loop\n",
    "    \n",
    "    for sampling_name, sampling in list(zip(sampling_names, samplings)):\n",
    "        X_resampled, y_resampled = sampling.fit_resample(train, y_train)\n",
    "        temp_results_df = evaluate_on_all_models(X_resampled, model_names, models, y_resampled)\n",
    "        sampling_name_list.extend([sampling_name] * row_length_per_loop)\n",
    "        results_df = pd.concat([results_df, temp_results_df], ignore_index=True)\n",
    "    \n",
    "    return pd.concat([results_df, pd.Series(sampling_name_list, name='sampling_method')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "380f37be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 8min 18s\n",
      "Wall time: 8min 16s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>test_average_precision</th>\n",
       "      <th>train_average_precision</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>sampling_method</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.998902</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.998264</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mixed_SMOTEEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>SGD Classifier</td>\n",
       "      <td>0.999964</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.998304</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.997317</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mixed_SMOTEEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.999943</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.979635</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.97918</td>\n",
       "      <td>1.0</td>\n",
       "      <td>oversampling_random</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.999887</td>\n",
       "      <td>0.999984</td>\n",
       "      <td>0.993453</td>\n",
       "      <td>0.997728</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.989583</td>\n",
       "      <td>0.996409</td>\n",
       "      <td>mixed_SMOTEEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.999881</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.99561</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.997997</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.993053</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mixed_SMOTEEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.999851</td>\n",
       "      <td>0.999961</td>\n",
       "      <td>0.946979</td>\n",
       "      <td>0.961067</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.911595</td>\n",
       "      <td>0.936109</td>\n",
       "      <td>mixed_SMOTEEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.998049</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.972632</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.972084</td>\n",
       "      <td>1.0</td>\n",
       "      <td>oversampling_ADASYN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.997541</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.970513</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.983584</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.970067</td>\n",
       "      <td>1.0</td>\n",
       "      <td>oversampling_SMOTE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.997461</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.971241</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.984984</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.970774</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mixed_SMOTETomek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>SGD Classifier</td>\n",
       "      <td>0.981908</td>\n",
       "      <td>0.998827</td>\n",
       "      <td>0.962239</td>\n",
       "      <td>0.989809</td>\n",
       "      <td>0.9982</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.96076</td>\n",
       "      <td>0.98969</td>\n",
       "      <td>oversampling_random</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SGD Classifier</td>\n",
       "      <td>0.981548</td>\n",
       "      <td>0.998195</td>\n",
       "      <td>0.95857</td>\n",
       "      <td>0.989499</td>\n",
       "      <td>0.989589</td>\n",
       "      <td>0.999049</td>\n",
       "      <td>0.957152</td>\n",
       "      <td>0.98939</td>\n",
       "      <td>oversampling_SMOTE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>SGD Classifier</td>\n",
       "      <td>0.981495</td>\n",
       "      <td>0.998254</td>\n",
       "      <td>0.960193</td>\n",
       "      <td>0.990062</td>\n",
       "      <td>0.99119</td>\n",
       "      <td>0.999099</td>\n",
       "      <td>0.958859</td>\n",
       "      <td>0.989965</td>\n",
       "      <td>mixed_SMOTETomek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SGD Classifier</td>\n",
       "      <td>0.979797</td>\n",
       "      <td>0.998093</td>\n",
       "      <td>0.96169</td>\n",
       "      <td>0.988373</td>\n",
       "      <td>0.9972</td>\n",
       "      <td>0.99985</td>\n",
       "      <td>0.960182</td>\n",
       "      <td>0.988219</td>\n",
       "      <td>oversampling_ADASYN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>0.979046</td>\n",
       "      <td>0.992273</td>\n",
       "      <td>0.960691</td>\n",
       "      <td>0.982002</td>\n",
       "      <td>0.993793</td>\n",
       "      <td>0.999249</td>\n",
       "      <td>0.959258</td>\n",
       "      <td>0.981682</td>\n",
       "      <td>mixed_SMOTETomek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>0.978159</td>\n",
       "      <td>0.992403</td>\n",
       "      <td>0.958924</td>\n",
       "      <td>0.981872</td>\n",
       "      <td>0.994192</td>\n",
       "      <td>0.998899</td>\n",
       "      <td>0.957356</td>\n",
       "      <td>0.981557</td>\n",
       "      <td>oversampling_SMOTE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>0.978035</td>\n",
       "      <td>0.992169</td>\n",
       "      <td>0.963354</td>\n",
       "      <td>0.982132</td>\n",
       "      <td>0.998599</td>\n",
       "      <td>0.99985</td>\n",
       "      <td>0.961964</td>\n",
       "      <td>0.981807</td>\n",
       "      <td>oversampling_random</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>0.97577</td>\n",
       "      <td>0.990233</td>\n",
       "      <td>0.962734</td>\n",
       "      <td>0.982176</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.961282</td>\n",
       "      <td>0.981841</td>\n",
       "      <td>oversampling_ADASYN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.969563</td>\n",
       "      <td>0.985321</td>\n",
       "      <td>0.940764</td>\n",
       "      <td>0.959642</td>\n",
       "      <td>0.967175</td>\n",
       "      <td>0.984184</td>\n",
       "      <td>0.939042</td>\n",
       "      <td>0.958609</td>\n",
       "      <td>mixed_SMOTETomek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.968642</td>\n",
       "      <td>0.985338</td>\n",
       "      <td>0.940601</td>\n",
       "      <td>0.959188</td>\n",
       "      <td>0.966371</td>\n",
       "      <td>0.983383</td>\n",
       "      <td>0.938941</td>\n",
       "      <td>0.958158</td>\n",
       "      <td>oversampling_SMOTE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.965428</td>\n",
       "      <td>0.977961</td>\n",
       "      <td>0.927291</td>\n",
       "      <td>0.942562</td>\n",
       "      <td>0.98939</td>\n",
       "      <td>0.992192</td>\n",
       "      <td>0.922321</td>\n",
       "      <td>0.93954</td>\n",
       "      <td>oversampling_random</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.965137</td>\n",
       "      <td>0.976384</td>\n",
       "      <td>0.929112</td>\n",
       "      <td>0.944644</td>\n",
       "      <td>0.988994</td>\n",
       "      <td>0.993644</td>\n",
       "      <td>0.924422</td>\n",
       "      <td>0.941767</td>\n",
       "      <td>oversampling_SMOTE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.964663</td>\n",
       "      <td>0.985399</td>\n",
       "      <td>0.944632</td>\n",
       "      <td>0.962696</td>\n",
       "      <td>0.978187</td>\n",
       "      <td>0.992692</td>\n",
       "      <td>0.942644</td>\n",
       "      <td>0.961537</td>\n",
       "      <td>oversampling_random</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.96424</td>\n",
       "      <td>0.97657</td>\n",
       "      <td>0.929608</td>\n",
       "      <td>0.944454</td>\n",
       "      <td>0.989994</td>\n",
       "      <td>0.993443</td>\n",
       "      <td>0.924923</td>\n",
       "      <td>0.941567</td>\n",
       "      <td>mixed_SMOTETomek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.958664</td>\n",
       "      <td>0.981816</td>\n",
       "      <td>0.946562</td>\n",
       "      <td>0.959841</td>\n",
       "      <td>0.9834</td>\n",
       "      <td>0.99125</td>\n",
       "      <td>0.944371</td>\n",
       "      <td>0.958504</td>\n",
       "      <td>oversampling_ADASYN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.958114</td>\n",
       "      <td>0.973546</td>\n",
       "      <td>0.927955</td>\n",
       "      <td>0.940382</td>\n",
       "      <td>0.9944</td>\n",
       "      <td>0.9964</td>\n",
       "      <td>0.92266</td>\n",
       "      <td>0.936793</td>\n",
       "      <td>oversampling_ADASYN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>SGD Classifier</td>\n",
       "      <td>0.933551</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.827352</td>\n",
       "      <td>0.998459</td>\n",
       "      <td>0.737425</td>\n",
       "      <td>0.996943</td>\n",
       "      <td>0.952939</td>\n",
       "      <td>0.999525</td>\n",
       "      <td>undersampling_All_KNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.923398</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.884121</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.903218</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.880982</td>\n",
       "      <td>1.0</td>\n",
       "      <td>undersampling_random</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>0.918805</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.846668</td>\n",
       "      <td>0.962185</td>\n",
       "      <td>0.847862</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.952524</td>\n",
       "      <td>0.987764</td>\n",
       "      <td>undersampling_All_KNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>SGD Classifier</td>\n",
       "      <td>0.915637</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8678</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.869103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.868042</td>\n",
       "      <td>1.0</td>\n",
       "      <td>undersampling_random</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.914714</td>\n",
       "      <td>0.985401</td>\n",
       "      <td>0.881691</td>\n",
       "      <td>0.945989</td>\n",
       "      <td>0.922713</td>\n",
       "      <td>0.994217</td>\n",
       "      <td>0.875593</td>\n",
       "      <td>0.943197</td>\n",
       "      <td>undersampling_random</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.913462</td>\n",
       "      <td>0.981495</td>\n",
       "      <td>0.632466</td>\n",
       "      <td>0.796976</td>\n",
       "      <td>0.468184</td>\n",
       "      <td>0.662935</td>\n",
       "      <td>0.916671</td>\n",
       "      <td>0.947733</td>\n",
       "      <td>undersampling_All_KNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>0.912334</td>\n",
       "      <td>0.998254</td>\n",
       "      <td>0.879067</td>\n",
       "      <td>0.975419</td>\n",
       "      <td>0.892736</td>\n",
       "      <td>0.991499</td>\n",
       "      <td>0.876856</td>\n",
       "      <td>0.975</td>\n",
       "      <td>undersampling_random</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.908815</td>\n",
       "      <td>0.982586</td>\n",
       "      <td>0.864132</td>\n",
       "      <td>0.942488</td>\n",
       "      <td>0.855678</td>\n",
       "      <td>0.956109</td>\n",
       "      <td>0.865272</td>\n",
       "      <td>0.941665</td>\n",
       "      <td>undersampling_random</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.889281</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.633254</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.481241</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.914765</td>\n",
       "      <td>1.0</td>\n",
       "      <td>undersampling_All_KNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.873404</td>\n",
       "      <td>0.962627</td>\n",
       "      <td>0.866636</td>\n",
       "      <td>0.941447</td>\n",
       "      <td>0.889977</td>\n",
       "      <td>0.9891</td>\n",
       "      <td>0.862501</td>\n",
       "      <td>0.938431</td>\n",
       "      <td>undersampling_Cluster_Centroids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.872155</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.833747</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.869379</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.826558</td>\n",
       "      <td>1.0</td>\n",
       "      <td>undersampling_Cluster_Centroids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>0.866713</td>\n",
       "      <td>0.990519</td>\n",
       "      <td>0.862074</td>\n",
       "      <td>0.957913</td>\n",
       "      <td>0.895448</td>\n",
       "      <td>0.990143</td>\n",
       "      <td>0.856423</td>\n",
       "      <td>0.956465</td>\n",
       "      <td>undersampling_Cluster_Centroids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.864752</td>\n",
       "      <td>0.968547</td>\n",
       "      <td>0.845958</td>\n",
       "      <td>0.929565</td>\n",
       "      <td>0.853241</td>\n",
       "      <td>0.960536</td>\n",
       "      <td>0.845506</td>\n",
       "      <td>0.927213</td>\n",
       "      <td>undersampling_Cluster_Centroids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>SGD Classifier</td>\n",
       "      <td>0.847431</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.845305</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.859356</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.843624</td>\n",
       "      <td>1.0</td>\n",
       "      <td>undersampling_Cluster_Centroids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.823461</td>\n",
       "      <td>0.930844</td>\n",
       "      <td>0.109079</td>\n",
       "      <td>0.172916</td>\n",
       "      <td>0.059908</td>\n",
       "      <td>0.09488</td>\n",
       "      <td>0.854218</td>\n",
       "      <td>0.859652</td>\n",
       "      <td>undersampling_All_KNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SGD Classifier</td>\n",
       "      <td>0.674751</td>\n",
       "      <td>0.996554</td>\n",
       "      <td>0.557209</td>\n",
       "      <td>0.979161</td>\n",
       "      <td>0.486299</td>\n",
       "      <td>0.968034</td>\n",
       "      <td>0.902623</td>\n",
       "      <td>0.994721</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.668113</td>\n",
       "      <td>0.870202</td>\n",
       "      <td>0.352067</td>\n",
       "      <td>0.500649</td>\n",
       "      <td>0.229931</td>\n",
       "      <td>0.342179</td>\n",
       "      <td>0.893016</td>\n",
       "      <td>0.912696</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.661767</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.30053</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.185103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.891977</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>0.642544</td>\n",
       "      <td>0.928927</td>\n",
       "      <td>0.638268</td>\n",
       "      <td>0.858607</td>\n",
       "      <td>0.723632</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.895451</td>\n",
       "      <td>0.957722</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.529009</td>\n",
       "      <td>0.765839</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001026</td>\n",
       "      <td>0.87173</td>\n",
       "      <td>0.871859</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model_name test_average_precision train_average_precision  \\\n",
       "36              Linear SVC               0.999989                     1.0   \n",
       "39          SGD Classifier               0.999964                     1.0   \n",
       "17  RandomForestClassifier               0.999943                     1.0   \n",
       "38     Logistic Regression               0.999887                0.999984   \n",
       "37  RandomForestClassifier               0.999881                     1.0   \n",
       "35             Naive Bayes               0.999851                0.999961   \n",
       "12  RandomForestClassifier               0.998049                     1.0   \n",
       "7   RandomForestClassifier               0.997541                     1.0   \n",
       "42  RandomForestClassifier               0.997461                     1.0   \n",
       "19          SGD Classifier               0.981908                0.998827   \n",
       "9           SGD Classifier               0.981548                0.998195   \n",
       "44          SGD Classifier               0.981495                0.998254   \n",
       "14          SGD Classifier               0.979797                0.998093   \n",
       "41              Linear SVC               0.979046                0.992273   \n",
       "6               Linear SVC               0.978159                0.992403   \n",
       "16              Linear SVC               0.978035                0.992169   \n",
       "11              Linear SVC                0.97577                0.990233   \n",
       "43     Logistic Regression               0.969563                0.985321   \n",
       "8      Logistic Regression               0.968642                0.985338   \n",
       "15             Naive Bayes               0.965428                0.977961   \n",
       "5              Naive Bayes               0.965137                0.976384   \n",
       "18     Logistic Regression               0.964663                0.985399   \n",
       "40             Naive Bayes                0.96424                 0.97657   \n",
       "13     Logistic Regression               0.958664                0.981816   \n",
       "10             Naive Bayes               0.958114                0.973546   \n",
       "29          SGD Classifier               0.933551                     1.0   \n",
       "32  RandomForestClassifier               0.923398                     1.0   \n",
       "26              Linear SVC               0.918805                0.999983   \n",
       "34          SGD Classifier               0.915637                     1.0   \n",
       "30             Naive Bayes               0.914714                0.985401   \n",
       "28     Logistic Regression               0.913462                0.981495   \n",
       "31              Linear SVC               0.912334                0.998254   \n",
       "33     Logistic Regression               0.908815                0.982586   \n",
       "27  RandomForestClassifier               0.889281                     1.0   \n",
       "20             Naive Bayes               0.873404                0.962627   \n",
       "22  RandomForestClassifier               0.872155                     1.0   \n",
       "21              Linear SVC               0.866713                0.990519   \n",
       "23     Logistic Regression               0.864752                0.968547   \n",
       "24          SGD Classifier               0.847431                     1.0   \n",
       "25             Naive Bayes               0.823461                0.930844   \n",
       "4           SGD Classifier               0.674751                0.996554   \n",
       "3      Logistic Regression               0.668113                0.870202   \n",
       "2   RandomForestClassifier               0.661767                     1.0   \n",
       "1               Linear SVC               0.642544                0.928927   \n",
       "0              Naive Bayes               0.529009                0.765839   \n",
       "\n",
       "     test_f1  train_f1 test_recall train_recall test_accuracy train_accuracy  \\\n",
       "36  0.998902       1.0      0.9998          1.0      0.998264            1.0   \n",
       "39  0.998304       1.0      0.9998          1.0      0.997317            1.0   \n",
       "17  0.979635       1.0         1.0          1.0       0.97918            1.0   \n",
       "38  0.993453  0.997728         1.0          1.0      0.989583       0.996409   \n",
       "37   0.99561       1.0    0.997997          1.0      0.993053            1.0   \n",
       "35  0.946979  0.961067         1.0          1.0      0.911595       0.936109   \n",
       "12  0.972632       1.0      0.9908          1.0      0.972084            1.0   \n",
       "7   0.970513       1.0    0.983584          1.0      0.970067            1.0   \n",
       "42  0.971241       1.0    0.984984          1.0      0.970774            1.0   \n",
       "19  0.962239  0.989809      0.9982       0.9998       0.96076        0.98969   \n",
       "9    0.95857  0.989499    0.989589     0.999049      0.957152        0.98939   \n",
       "44  0.960193  0.990062     0.99119     0.999099      0.958859       0.989965   \n",
       "14   0.96169  0.988373      0.9972      0.99985      0.960182       0.988219   \n",
       "41  0.960691  0.982002    0.993793     0.999249      0.959258       0.981682   \n",
       "6   0.958924  0.981872    0.994192     0.998899      0.957356       0.981557   \n",
       "16  0.963354  0.982132    0.998599      0.99985      0.961964       0.981807   \n",
       "11  0.962734  0.982176       0.999          1.0      0.961282       0.981841   \n",
       "43  0.940764  0.959642    0.967175     0.984184      0.939042       0.958609   \n",
       "8   0.940601  0.959188    0.966371     0.983383      0.938941       0.958158   \n",
       "15  0.927291  0.942562     0.98939     0.992192      0.922321        0.93954   \n",
       "5   0.929112  0.944644    0.988994     0.993644      0.924422       0.941767   \n",
       "18  0.944632  0.962696    0.978187     0.992692      0.942644       0.961537   \n",
       "40  0.929608  0.944454    0.989994     0.993443      0.924923       0.941567   \n",
       "13  0.946562  0.959841      0.9834      0.99125      0.944371       0.958504   \n",
       "10  0.927955  0.940382      0.9944       0.9964       0.92266       0.936793   \n",
       "29  0.827352  0.998459    0.737425     0.996943      0.952939       0.999525   \n",
       "32  0.884121       1.0    0.903218          1.0      0.880982            1.0   \n",
       "26  0.846668  0.962185    0.847862          1.0      0.952524       0.987764   \n",
       "34    0.8678       1.0    0.869103          1.0      0.868042            1.0   \n",
       "30  0.881691  0.945989    0.922713     0.994217      0.875593       0.943197   \n",
       "28  0.632466  0.796976    0.468184     0.662935      0.916671       0.947733   \n",
       "31  0.879067  0.975419    0.892736     0.991499      0.876856          0.975   \n",
       "33  0.864132  0.942488    0.855678     0.956109      0.865272       0.941665   \n",
       "27  0.633254       1.0    0.481241          1.0      0.914765            1.0   \n",
       "20  0.866636  0.941447    0.889977       0.9891      0.862501       0.938431   \n",
       "22  0.833747       1.0    0.869379          1.0      0.826558            1.0   \n",
       "21  0.862074  0.957913    0.895448     0.990143      0.856423       0.956465   \n",
       "23  0.845958  0.929565    0.853241     0.960536      0.845506       0.927213   \n",
       "24  0.845305       1.0    0.859356          1.0      0.843624            1.0   \n",
       "25  0.109079  0.172916    0.059908      0.09488      0.854218       0.859652   \n",
       "4   0.557209  0.979161    0.486299     0.968034      0.902623       0.994721   \n",
       "3   0.352067  0.500649    0.229931     0.342179      0.893016       0.912696   \n",
       "2    0.30053       1.0    0.185103          1.0      0.891977            1.0   \n",
       "1   0.638268  0.858607    0.723632          1.0      0.895451       0.957722   \n",
       "0        0.0     0.002         0.0     0.001026       0.87173       0.871859   \n",
       "\n",
       "                    sampling_method  \n",
       "36                    mixed_SMOTEEN  \n",
       "39                    mixed_SMOTEEN  \n",
       "17              oversampling_random  \n",
       "38                    mixed_SMOTEEN  \n",
       "37                    mixed_SMOTEEN  \n",
       "35                    mixed_SMOTEEN  \n",
       "12              oversampling_ADASYN  \n",
       "7                oversampling_SMOTE  \n",
       "42                 mixed_SMOTETomek  \n",
       "19              oversampling_random  \n",
       "9                oversampling_SMOTE  \n",
       "44                 mixed_SMOTETomek  \n",
       "14              oversampling_ADASYN  \n",
       "41                 mixed_SMOTETomek  \n",
       "6                oversampling_SMOTE  \n",
       "16              oversampling_random  \n",
       "11              oversampling_ADASYN  \n",
       "43                 mixed_SMOTETomek  \n",
       "8                oversampling_SMOTE  \n",
       "15              oversampling_random  \n",
       "5                oversampling_SMOTE  \n",
       "18              oversampling_random  \n",
       "40                 mixed_SMOTETomek  \n",
       "13              oversampling_ADASYN  \n",
       "10              oversampling_ADASYN  \n",
       "29            undersampling_All_KNN  \n",
       "32             undersampling_random  \n",
       "26            undersampling_All_KNN  \n",
       "34             undersampling_random  \n",
       "30             undersampling_random  \n",
       "28            undersampling_All_KNN  \n",
       "31             undersampling_random  \n",
       "33             undersampling_random  \n",
       "27            undersampling_All_KNN  \n",
       "20  undersampling_Cluster_Centroids  \n",
       "22  undersampling_Cluster_Centroids  \n",
       "21  undersampling_Cluster_Centroids  \n",
       "23  undersampling_Cluster_Centroids  \n",
       "24  undersampling_Cluster_Centroids  \n",
       "25            undersampling_All_KNN  \n",
       "4                              None  \n",
       "3                              None  \n",
       "2                              None  \n",
       "1                              None  \n",
       "0                              None  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Evaluate all the 5 models on all of the sampling possibilities to check which one performs best, while using the text data\n",
    "# with manual features.\n",
    "with_manual_df = evaluate_balancing(sampling_method_names, sampling_methods, model_names, models, X_train, True)\n",
    "with_manual_df.sort_values(by=['test_average_precision', 'test_f1', 'test_recall', 'test_accuracy'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e8751d14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 7min 50s\n",
      "Wall time: 7min 51s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>test_average_precision</th>\n",
       "      <th>train_average_precision</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>sampling_method</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999401</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999046</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mixed_SMOTEEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>SGD Classifier</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.99857</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mixed_SMOTEEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.996514</td>\n",
       "      <td>0.999199</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.994434</td>\n",
       "      <td>0.998727</td>\n",
       "      <td>mixed_SMOTEEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.999973</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.996699</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.997997</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.994749</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mixed_SMOTEEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.999971</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.950628</td>\n",
       "      <td>0.970087</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.917423</td>\n",
       "      <td>0.951034</td>\n",
       "      <td>mixed_SMOTEEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.999868</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.975426</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999198</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.974774</td>\n",
       "      <td>1.0</td>\n",
       "      <td>oversampling_random</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.997845</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.968201</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.994563</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.96678</td>\n",
       "      <td>1.0</td>\n",
       "      <td>oversampling_ADASYN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.997113</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.967231</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990791</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.966369</td>\n",
       "      <td>1.0</td>\n",
       "      <td>oversampling_SMOTE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.996964</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.966195</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.989791</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.965261</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mixed_SMOTETomek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>SGD Classifier</td>\n",
       "      <td>0.985193</td>\n",
       "      <td>0.998338</td>\n",
       "      <td>0.960896</td>\n",
       "      <td>0.987911</td>\n",
       "      <td>0.992593</td>\n",
       "      <td>0.999449</td>\n",
       "      <td>0.959558</td>\n",
       "      <td>0.987763</td>\n",
       "      <td>mixed_SMOTETomek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SGD Classifier</td>\n",
       "      <td>0.984503</td>\n",
       "      <td>0.998523</td>\n",
       "      <td>0.960541</td>\n",
       "      <td>0.98752</td>\n",
       "      <td>0.992993</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.95916</td>\n",
       "      <td>0.987362</td>\n",
       "      <td>oversampling_SMOTE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>SGD Classifier</td>\n",
       "      <td>0.983596</td>\n",
       "      <td>0.998983</td>\n",
       "      <td>0.962535</td>\n",
       "      <td>0.988792</td>\n",
       "      <td>0.998397</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>0.961056</td>\n",
       "      <td>0.988664</td>\n",
       "      <td>oversampling_random</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>0.980794</td>\n",
       "      <td>0.992196</td>\n",
       "      <td>0.957246</td>\n",
       "      <td>0.98019</td>\n",
       "      <td>0.992983</td>\n",
       "      <td>0.999049</td>\n",
       "      <td>0.955555</td>\n",
       "      <td>0.979805</td>\n",
       "      <td>mixed_SMOTETomek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SGD Classifier</td>\n",
       "      <td>0.979837</td>\n",
       "      <td>0.998055</td>\n",
       "      <td>0.963143</td>\n",
       "      <td>0.987137</td>\n",
       "      <td>0.998641</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.961063</td>\n",
       "      <td>0.986767</td>\n",
       "      <td>oversampling_ADASYN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>0.978381</td>\n",
       "      <td>0.992346</td>\n",
       "      <td>0.960119</td>\n",
       "      <td>0.979945</td>\n",
       "      <td>0.996597</td>\n",
       "      <td>0.998899</td>\n",
       "      <td>0.958559</td>\n",
       "      <td>0.979555</td>\n",
       "      <td>oversampling_SMOTE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>0.977535</td>\n",
       "      <td>0.989689</td>\n",
       "      <td>0.961817</td>\n",
       "      <td>0.981257</td>\n",
       "      <td>0.999223</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.959685</td>\n",
       "      <td>0.980606</td>\n",
       "      <td>oversampling_ADASYN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>0.977294</td>\n",
       "      <td>0.991267</td>\n",
       "      <td>0.960117</td>\n",
       "      <td>0.980434</td>\n",
       "      <td>0.996398</td>\n",
       "      <td>0.999249</td>\n",
       "      <td>0.958559</td>\n",
       "      <td>0.980055</td>\n",
       "      <td>oversampling_random</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.975705</td>\n",
       "      <td>0.984672</td>\n",
       "      <td>0.932091</td>\n",
       "      <td>0.946085</td>\n",
       "      <td>0.98899</td>\n",
       "      <td>0.993694</td>\n",
       "      <td>0.927828</td>\n",
       "      <td>0.943368</td>\n",
       "      <td>oversampling_SMOTE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.975648</td>\n",
       "      <td>0.985858</td>\n",
       "      <td>0.931112</td>\n",
       "      <td>0.948006</td>\n",
       "      <td>0.990791</td>\n",
       "      <td>0.997397</td>\n",
       "      <td>0.926627</td>\n",
       "      <td>0.945295</td>\n",
       "      <td>oversampling_random</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.975005</td>\n",
       "      <td>0.984782</td>\n",
       "      <td>0.931368</td>\n",
       "      <td>0.946225</td>\n",
       "      <td>0.987993</td>\n",
       "      <td>0.993343</td>\n",
       "      <td>0.927124</td>\n",
       "      <td>0.943543</td>\n",
       "      <td>mixed_SMOTETomek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.969375</td>\n",
       "      <td>0.985171</td>\n",
       "      <td>0.943906</td>\n",
       "      <td>0.955726</td>\n",
       "      <td>0.971167</td>\n",
       "      <td>0.978779</td>\n",
       "      <td>0.942138</td>\n",
       "      <td>0.954655</td>\n",
       "      <td>mixed_SMOTETomek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.969274</td>\n",
       "      <td>0.985177</td>\n",
       "      <td>0.942229</td>\n",
       "      <td>0.956148</td>\n",
       "      <td>0.969372</td>\n",
       "      <td>0.978829</td>\n",
       "      <td>0.940539</td>\n",
       "      <td>0.955105</td>\n",
       "      <td>oversampling_SMOTE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.967668</td>\n",
       "      <td>0.979786</td>\n",
       "      <td>0.930402</td>\n",
       "      <td>0.9445</td>\n",
       "      <td>0.999417</td>\n",
       "      <td>0.999951</td>\n",
       "      <td>0.924004</td>\n",
       "      <td>0.94034</td>\n",
       "      <td>oversampling_ADASYN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.967153</td>\n",
       "      <td>0.985193</td>\n",
       "      <td>0.947422</td>\n",
       "      <td>0.962362</td>\n",
       "      <td>0.983779</td>\n",
       "      <td>0.993694</td>\n",
       "      <td>0.945349</td>\n",
       "      <td>0.961136</td>\n",
       "      <td>oversampling_random</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.961595</td>\n",
       "      <td>0.97922</td>\n",
       "      <td>0.949</td>\n",
       "      <td>0.961014</td>\n",
       "      <td>0.986019</td>\n",
       "      <td>0.994369</td>\n",
       "      <td>0.946185</td>\n",
       "      <td>0.959044</td>\n",
       "      <td>oversampling_ADASYN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>SGD Classifier</td>\n",
       "      <td>0.946106</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.852065</td>\n",
       "      <td>0.999488</td>\n",
       "      <td>0.774759</td>\n",
       "      <td>0.99898</td>\n",
       "      <td>0.956732</td>\n",
       "      <td>0.999834</td>\n",
       "      <td>undersampling_All_KNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.933978</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.879801</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.930391</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.871935</td>\n",
       "      <td>1.0</td>\n",
       "      <td>undersampling_random</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>0.931964</td>\n",
       "      <td>0.999538</td>\n",
       "      <td>0.859013</td>\n",
       "      <td>0.970817</td>\n",
       "      <td>0.833701</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.955373</td>\n",
       "      <td>0.990177</td>\n",
       "      <td>undersampling_All_KNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.931759</td>\n",
       "      <td>0.99074</td>\n",
       "      <td>0.885948</td>\n",
       "      <td>0.954978</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.995231</td>\n",
       "      <td>0.879556</td>\n",
       "      <td>0.953061</td>\n",
       "      <td>undersampling_random</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>SGD Classifier</td>\n",
       "      <td>0.927561</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.886424</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.895264</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.885085</td>\n",
       "      <td>1.0</td>\n",
       "      <td>undersampling_random</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.925443</td>\n",
       "      <td>0.98346</td>\n",
       "      <td>0.600565</td>\n",
       "      <td>0.792246</td>\n",
       "      <td>0.441379</td>\n",
       "      <td>0.658496</td>\n",
       "      <td>0.906339</td>\n",
       "      <td>0.94373</td>\n",
       "      <td>undersampling_All_KNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.92009</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.772314</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.648736</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.93874</td>\n",
       "      <td>1.0</td>\n",
       "      <td>undersampling_All_KNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>0.917448</td>\n",
       "      <td>0.997677</td>\n",
       "      <td>0.887055</td>\n",
       "      <td>0.973399</td>\n",
       "      <td>0.907494</td>\n",
       "      <td>0.99557</td>\n",
       "      <td>0.885026</td>\n",
       "      <td>0.972783</td>\n",
       "      <td>undersampling_random</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.916243</td>\n",
       "      <td>0.985599</td>\n",
       "      <td>0.875175</td>\n",
       "      <td>0.946654</td>\n",
       "      <td>0.87223</td>\n",
       "      <td>0.962923</td>\n",
       "      <td>0.876201</td>\n",
       "      <td>0.945746</td>\n",
       "      <td>undersampling_random</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.899725</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.870925</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.910391</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.864605</td>\n",
       "      <td>1.0</td>\n",
       "      <td>undersampling_Cluster_Centroids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.894818</td>\n",
       "      <td>0.982192</td>\n",
       "      <td>0.890191</td>\n",
       "      <td>0.942021</td>\n",
       "      <td>0.941379</td>\n",
       "      <td>0.991494</td>\n",
       "      <td>0.883717</td>\n",
       "      <td>0.938947</td>\n",
       "      <td>undersampling_Cluster_Centroids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>SGD Classifier</td>\n",
       "      <td>0.883499</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.835464</td>\n",
       "      <td>0.999662</td>\n",
       "      <td>0.841057</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.835441</td>\n",
       "      <td>0.99966</td>\n",
       "      <td>undersampling_Cluster_Centroids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.880759</td>\n",
       "      <td>0.977716</td>\n",
       "      <td>0.856578</td>\n",
       "      <td>0.942384</td>\n",
       "      <td>0.861103</td>\n",
       "      <td>0.978586</td>\n",
       "      <td>0.855815</td>\n",
       "      <td>0.94014</td>\n",
       "      <td>undersampling_Cluster_Centroids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>0.876429</td>\n",
       "      <td>0.989991</td>\n",
       "      <td>0.8726</td>\n",
       "      <td>0.956363</td>\n",
       "      <td>0.903494</td>\n",
       "      <td>0.99796</td>\n",
       "      <td>0.868732</td>\n",
       "      <td>0.954419</td>\n",
       "      <td>undersampling_Cluster_Centroids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.865789</td>\n",
       "      <td>0.957761</td>\n",
       "      <td>0.171303</td>\n",
       "      <td>0.306431</td>\n",
       "      <td>0.09669</td>\n",
       "      <td>0.181281</td>\n",
       "      <td>0.851945</td>\n",
       "      <td>0.866426</td>\n",
       "      <td>undersampling_All_KNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SGD Classifier</td>\n",
       "      <td>0.708524</td>\n",
       "      <td>0.997671</td>\n",
       "      <td>0.569661</td>\n",
       "      <td>0.976792</td>\n",
       "      <td>0.480184</td>\n",
       "      <td>0.960545</td>\n",
       "      <td>0.908728</td>\n",
       "      <td>0.994154</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.680848</td>\n",
       "      <td>0.877513</td>\n",
       "      <td>0.29441</td>\n",
       "      <td>0.46052</td>\n",
       "      <td>0.183586</td>\n",
       "      <td>0.302364</td>\n",
       "      <td>0.889703</td>\n",
       "      <td>0.909293</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.673176</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.339826</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.219264</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.893715</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>0.64928</td>\n",
       "      <td>0.925489</td>\n",
       "      <td>0.63489</td>\n",
       "      <td>0.857634</td>\n",
       "      <td>0.727632</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.892846</td>\n",
       "      <td>0.957329</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.538275</td>\n",
       "      <td>0.769662</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02129</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010874</td>\n",
       "      <td>0.869111</td>\n",
       "      <td>0.872295</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model_name test_average_precision train_average_precision  \\\n",
       "36              Linear SVC                    1.0                     1.0   \n",
       "39          SGD Classifier               0.999997                     1.0   \n",
       "38     Logistic Regression               0.999985                0.999999   \n",
       "37  RandomForestClassifier               0.999973                     1.0   \n",
       "35             Naive Bayes               0.999971                0.999996   \n",
       "17  RandomForestClassifier               0.999868                     1.0   \n",
       "12  RandomForestClassifier               0.997845                     1.0   \n",
       "7   RandomForestClassifier               0.997113                     1.0   \n",
       "42  RandomForestClassifier               0.996964                     1.0   \n",
       "44          SGD Classifier               0.985193                0.998338   \n",
       "9           SGD Classifier               0.984503                0.998523   \n",
       "19          SGD Classifier               0.983596                0.998983   \n",
       "41              Linear SVC               0.980794                0.992196   \n",
       "14          SGD Classifier               0.979837                0.998055   \n",
       "6               Linear SVC               0.978381                0.992346   \n",
       "11              Linear SVC               0.977535                0.989689   \n",
       "16              Linear SVC               0.977294                0.991267   \n",
       "5              Naive Bayes               0.975705                0.984672   \n",
       "15             Naive Bayes               0.975648                0.985858   \n",
       "40             Naive Bayes               0.975005                0.984782   \n",
       "43     Logistic Regression               0.969375                0.985171   \n",
       "8      Logistic Regression               0.969274                0.985177   \n",
       "10             Naive Bayes               0.967668                0.979786   \n",
       "18     Logistic Regression               0.967153                0.985193   \n",
       "13     Logistic Regression               0.961595                 0.97922   \n",
       "29          SGD Classifier               0.946106                     1.0   \n",
       "32  RandomForestClassifier               0.933978                     1.0   \n",
       "26              Linear SVC               0.931964                0.999538   \n",
       "30             Naive Bayes               0.931759                 0.99074   \n",
       "34          SGD Classifier               0.927561                     1.0   \n",
       "28     Logistic Regression               0.925443                 0.98346   \n",
       "27  RandomForestClassifier                0.92009                     1.0   \n",
       "31              Linear SVC               0.917448                0.997677   \n",
       "33     Logistic Regression               0.916243                0.985599   \n",
       "22  RandomForestClassifier               0.899725                     1.0   \n",
       "20             Naive Bayes               0.894818                0.982192   \n",
       "24          SGD Classifier               0.883499                     1.0   \n",
       "23     Logistic Regression               0.880759                0.977716   \n",
       "21              Linear SVC               0.876429                0.989991   \n",
       "25             Naive Bayes               0.865789                0.957761   \n",
       "4           SGD Classifier               0.708524                0.997671   \n",
       "3      Logistic Regression               0.680848                0.877513   \n",
       "2   RandomForestClassifier               0.673176                     1.0   \n",
       "1               Linear SVC                0.64928                0.925489   \n",
       "0              Naive Bayes               0.538275                0.769662   \n",
       "\n",
       "     test_f1  train_f1 test_recall train_recall test_accuracy train_accuracy  \\\n",
       "36  0.999401       1.0         1.0          1.0      0.999046            1.0   \n",
       "39  0.999103       1.0         1.0          1.0       0.99857            1.0   \n",
       "38  0.996514  0.999199         1.0          1.0      0.994434       0.998727   \n",
       "37  0.996699       1.0    0.997997          1.0      0.994749            1.0   \n",
       "35  0.950628  0.970087         1.0          1.0      0.917423       0.951034   \n",
       "17  0.975426       1.0    0.999198          1.0      0.974774            1.0   \n",
       "12  0.968201       1.0    0.994563          1.0       0.96678            1.0   \n",
       "7   0.967231       1.0    0.990791          1.0      0.966369            1.0   \n",
       "42  0.966195       1.0    0.989791          1.0      0.965261            1.0   \n",
       "44  0.960896  0.987911    0.992593     0.999449      0.959558       0.987763   \n",
       "9   0.960541   0.98752    0.992993       0.9997       0.95916       0.987362   \n",
       "19  0.962535  0.988792    0.998397       0.9996      0.961056       0.988664   \n",
       "41  0.957246   0.98019    0.992983     0.999049      0.955555       0.979805   \n",
       "14  0.963143  0.987137    0.998641          1.0      0.961063       0.986767   \n",
       "6   0.960119  0.979945    0.996597     0.998899      0.958559       0.979555   \n",
       "11  0.961817  0.981257    0.999223          1.0      0.959685       0.980606   \n",
       "16  0.960117  0.980434    0.996398     0.999249      0.958559       0.980055   \n",
       "5   0.932091  0.946085     0.98899     0.993694      0.927828       0.943368   \n",
       "15  0.931112  0.948006    0.990791     0.997397      0.926627       0.945295   \n",
       "40  0.931368  0.946225    0.987993     0.993343      0.927124       0.943543   \n",
       "43  0.943906  0.955726    0.971167     0.978779      0.942138       0.954655   \n",
       "8   0.942229  0.956148    0.969372     0.978829      0.940539       0.955105   \n",
       "10  0.930402    0.9445    0.999417     0.999951      0.924004        0.94034   \n",
       "18  0.947422  0.962362    0.983779     0.993694      0.945349       0.961136   \n",
       "13     0.949  0.961014    0.986019     0.994369      0.946185       0.959044   \n",
       "29  0.852065  0.999488    0.774759      0.99898      0.956732       0.999834   \n",
       "32  0.879801       1.0    0.930391          1.0      0.871935            1.0   \n",
       "26  0.859013  0.970817    0.833701          1.0      0.955373       0.990177   \n",
       "30  0.885948  0.954978    0.933333     0.995231      0.879556       0.953061   \n",
       "34  0.886424       1.0    0.895264          1.0      0.885085            1.0   \n",
       "28  0.600565  0.792246    0.441379     0.658496      0.906339        0.94373   \n",
       "27  0.772314       1.0    0.648736          1.0       0.93874            1.0   \n",
       "31  0.887055  0.973399    0.907494      0.99557      0.885026       0.972783   \n",
       "33  0.875175  0.946654     0.87223     0.962923      0.876201       0.945746   \n",
       "22  0.870925       1.0    0.910391          1.0      0.864605            1.0   \n",
       "20  0.890191  0.942021    0.941379     0.991494      0.883717       0.938947   \n",
       "24  0.835464  0.999662    0.841057          1.0      0.835441        0.99966   \n",
       "23  0.856578  0.942384    0.861103     0.978586      0.855815        0.94014   \n",
       "21    0.8726  0.956363    0.903494      0.99796      0.868732       0.954419   \n",
       "25  0.171303  0.306431     0.09669     0.181281      0.851945       0.866426   \n",
       "4   0.569661  0.976792    0.480184     0.960545      0.908728       0.994154   \n",
       "3    0.29441   0.46052    0.183586     0.302364      0.889703       0.909293   \n",
       "2   0.339826       1.0    0.219264          1.0      0.893715            1.0   \n",
       "1    0.63489  0.857634    0.727632          1.0      0.892846       0.957329   \n",
       "0        0.0   0.02129         0.0     0.010874      0.869111       0.872295   \n",
       "\n",
       "                    sampling_method  \n",
       "36                    mixed_SMOTEEN  \n",
       "39                    mixed_SMOTEEN  \n",
       "38                    mixed_SMOTEEN  \n",
       "37                    mixed_SMOTEEN  \n",
       "35                    mixed_SMOTEEN  \n",
       "17              oversampling_random  \n",
       "12              oversampling_ADASYN  \n",
       "7                oversampling_SMOTE  \n",
       "42                 mixed_SMOTETomek  \n",
       "44                 mixed_SMOTETomek  \n",
       "9                oversampling_SMOTE  \n",
       "19              oversampling_random  \n",
       "41                 mixed_SMOTETomek  \n",
       "14              oversampling_ADASYN  \n",
       "6                oversampling_SMOTE  \n",
       "11              oversampling_ADASYN  \n",
       "16              oversampling_random  \n",
       "5                oversampling_SMOTE  \n",
       "15              oversampling_random  \n",
       "40                 mixed_SMOTETomek  \n",
       "43                 mixed_SMOTETomek  \n",
       "8                oversampling_SMOTE  \n",
       "10              oversampling_ADASYN  \n",
       "18              oversampling_random  \n",
       "13              oversampling_ADASYN  \n",
       "29            undersampling_All_KNN  \n",
       "32             undersampling_random  \n",
       "26            undersampling_All_KNN  \n",
       "30             undersampling_random  \n",
       "34             undersampling_random  \n",
       "28            undersampling_All_KNN  \n",
       "27            undersampling_All_KNN  \n",
       "31             undersampling_random  \n",
       "33             undersampling_random  \n",
       "22  undersampling_Cluster_Centroids  \n",
       "20  undersampling_Cluster_Centroids  \n",
       "24  undersampling_Cluster_Centroids  \n",
       "23  undersampling_Cluster_Centroids  \n",
       "21  undersampling_Cluster_Centroids  \n",
       "25            undersampling_All_KNN  \n",
       "4                              None  \n",
       "3                              None  \n",
       "2                              None  \n",
       "1                              None  \n",
       "0                              None  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Evaluate all the 5 models on all of the sampling possibilities to check which one performs best, while using the text data\n",
    "# without manual features.\n",
    "wo_manual_df = evaluate_balancing(sampling_method_names, sampling_methods, model_names, models, X_train, False)\n",
    "wo_manual_df.sort_values(by=['test_average_precision', 'test_f1', 'test_recall', 'test_accuracy'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d264ed56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>test_average_precision</th>\n",
       "      <th>train_average_precision</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>sampling_method</th>\n",
       "      <th>m_feats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999401</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999046</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mixed_SMOTEEN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>SGD Classifier</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.99857</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mixed_SMOTEEN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.998902</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.998264</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mixed_SMOTEEN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.996514</td>\n",
       "      <td>0.999199</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.994434</td>\n",
       "      <td>0.998727</td>\n",
       "      <td>mixed_SMOTEEN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.999973</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.996699</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.997997</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.994749</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mixed_SMOTEEN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.661767</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.30053</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.185103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.891977</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>0.64928</td>\n",
       "      <td>0.925489</td>\n",
       "      <td>0.63489</td>\n",
       "      <td>0.857634</td>\n",
       "      <td>0.727632</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.892846</td>\n",
       "      <td>0.957329</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>0.642544</td>\n",
       "      <td>0.928927</td>\n",
       "      <td>0.638268</td>\n",
       "      <td>0.858607</td>\n",
       "      <td>0.723632</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.895451</td>\n",
       "      <td>0.957722</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.538275</td>\n",
       "      <td>0.769662</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02129</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010874</td>\n",
       "      <td>0.869111</td>\n",
       "      <td>0.872295</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.529009</td>\n",
       "      <td>0.765839</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001026</td>\n",
       "      <td>0.87173</td>\n",
       "      <td>0.871859</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows  11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                model_name test_average_precision train_average_precision  \\\n",
       "81              Linear SVC                    1.0                     1.0   \n",
       "84          SGD Classifier               0.999997                     1.0   \n",
       "36              Linear SVC               0.999989                     1.0   \n",
       "83     Logistic Regression               0.999985                0.999999   \n",
       "82  RandomForestClassifier               0.999973                     1.0   \n",
       "..                     ...                    ...                     ...   \n",
       "2   RandomForestClassifier               0.661767                     1.0   \n",
       "46              Linear SVC                0.64928                0.925489   \n",
       "1               Linear SVC               0.642544                0.928927   \n",
       "45             Naive Bayes               0.538275                0.769662   \n",
       "0              Naive Bayes               0.529009                0.765839   \n",
       "\n",
       "     test_f1  train_f1 test_recall train_recall test_accuracy train_accuracy  \\\n",
       "81  0.999401       1.0         1.0          1.0      0.999046            1.0   \n",
       "84  0.999103       1.0         1.0          1.0       0.99857            1.0   \n",
       "36  0.998902       1.0      0.9998          1.0      0.998264            1.0   \n",
       "83  0.996514  0.999199         1.0          1.0      0.994434       0.998727   \n",
       "82  0.996699       1.0    0.997997          1.0      0.994749            1.0   \n",
       "..       ...       ...         ...          ...           ...            ...   \n",
       "2    0.30053       1.0    0.185103          1.0      0.891977            1.0   \n",
       "46   0.63489  0.857634    0.727632          1.0      0.892846       0.957329   \n",
       "1   0.638268  0.858607    0.723632          1.0      0.895451       0.957722   \n",
       "45       0.0   0.02129         0.0     0.010874      0.869111       0.872295   \n",
       "0        0.0     0.002         0.0     0.001026       0.87173       0.871859   \n",
       "\n",
       "   sampling_method  m_feats  \n",
       "81   mixed_SMOTEEN    False  \n",
       "84   mixed_SMOTEEN    False  \n",
       "36   mixed_SMOTEEN     True  \n",
       "83   mixed_SMOTEEN    False  \n",
       "82   mixed_SMOTEEN    False  \n",
       "..             ...      ...  \n",
       "2             None     True  \n",
       "46            None    False  \n",
       "1             None     True  \n",
       "45            None    False  \n",
       "0             None     True  \n",
       "\n",
       "[90 rows x 11 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add column to differentiate between w/ and w/o manual features\n",
    "with_manual_df['m_feats'] = True\n",
    "wo_manual_df['m_feats'] = False\n",
    "all_results_df = pd.concat([with_manual_df, wo_manual_df], ignore_index=True)\n",
    "all_results_df.sort_values(by=['test_average_precision', 'test_f1', 'test_recall', 'test_accuracy'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1264e5da",
   "metadata": {},
   "source": [
    "## Evaluating the models\n",
    "\n",
    "Now that all the models have been evaluated on the train set through cross-validation, we can check how the best models are doing on our hold out test set and then pick one of them to go forward with.\n",
    "\n",
    "Our top 5 models are:\n",
    "\n",
    "model_name\tsampling_method\tm_feats \\\n",
    "Linear SVC\tmixed_SMOTEEN False \\\n",
    "SGD Classifier\tmixed_SMOTEEN False \\\n",
    "Linear SVC\tmixed_SMOTEEN True \\\n",
    "Logistic Regressionr  mixed_SMOTEEN\tFalse \\\n",
    "Random Forest Classifier  mixed_SMOTEEN\tFalse \\\n",
    "\n",
    "Therefore, we will fit these 5 models again on the given datasets and then compare their results and choose the best one for later predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b19a5d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the augmented dataset and scalers/vectorizers\n",
    "\n",
    "# SMOTEEN with manual features\n",
    "text_train, m_feats_train = get_text_m_feats(X_train, new_token_dict, True)\n",
    "\n",
    "tfidf_vectorizer_with_mf = get_tfidf_vectorizer(text_train)\n",
    "tfidf_train = tfidf_vectorizer_with_mf.transform(text_train)\n",
    "\n",
    "scaler_with_mf = get_scaler(m_feats_train)\n",
    "scaled_m_feats_train = pd.DataFrame(scaler_with_mf.transform(m_feats_train), index = m_feats_train.index.values)\n",
    "\n",
    "train_with_mf = hstack((tfidf_train, csr_matrix(scaled_m_feats_train)))\n",
    "\n",
    "X_SMOTEEN_with_mf, y_SMOTEEN_with_mf = SMOTEENN(random_state=42).fit_resample(train_with_mf, y_train)\n",
    "\n",
    "\n",
    "# SMOTEENN without manual features\n",
    "text_train, _ = get_text_m_feats(X_train, new_token_dict, False)\n",
    "\n",
    "tfidf_vectorizer_wo_mf = get_tfidf_vectorizer(text_train)\n",
    "tfidf_train_wo_mf = tfidf_vectorizer_wo_mf.transform(text_train)\n",
    "\n",
    "X_SMOTEEN_wo_mf, y_SMOTEEN_wo_mf = SMOTEENN(random_state=42).fit_resample(tfidf_train_wo_mf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4ed478ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process test data for both with and without manual features for testing purposes\n",
    "X_test_with_mf = transform_to_tfidf_matrix(\n",
    "    new_token_dict, tfidf_vectorizer_with_mf, scaler_with_mf, X_test, True)\n",
    "X_test_wo_mf = transform_to_tfidf_matrix(new_token_dict, tfidf_vectorizer_wo_mf, None, X_test, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0e01cbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 5 models that performed the best (3 will be created actually, but 2 will be fitted on 2 types of datasets)\n",
    "svc = SVC(random_state=42, class_weight='balanced', probability=True, kernel='linear')\n",
    "sgd = SGDClassifier(random_state=42, loss='log_loss')\n",
    "rf = RandomForestClassifier(n_estimators=150, random_state=42)\n",
    "lr = LogisticRegression(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5be0bda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to fit a model on a training set and evaluate it on a test set\n",
    "def fit_and_evaluate(train_set, y_train, test_set, y_test, model_name, model):\n",
    "    \"\"\"\n",
    "    Function which trains a model on a train set and builds a dataframe containing a list of metrics based on testing\n",
    "    the model on the test data.\n",
    "    :param train_set: The train dataset. It must be a pandas.Series containing rows of text.\n",
    "    :param y_train: The target values in the train set used for fitting the model.\n",
    "    :param test_set: The test dataset. It must be a pandas.Series containing rows of text.\n",
    "    :param y_test: The target values in the test set used for evaluating the reliability of the model.\n",
    "    :param model_name: A string representing the name of the model to be trained and evaluated.\n",
    "    :param model: The scikit-learn model that will be trained on the train data.\n",
    "    :return: a list of 6 items representing: \n",
    "    [model name, precision-recall-auc score of the model, f1 score of the positive class, \n",
    "    recall score of the positive class, accuracy score]\n",
    "    \"\"\"\n",
    "    model.fit(train_set, y_train)\n",
    "    predicted = model.predict(test_set)\n",
    "\n",
    "    acc = accuracy_score(y_test, predicted)\n",
    "    f1_sc = f1_score(y_test, predicted, average=None)\n",
    "    ap_auc = average_precision_score(y_test, model.predict_proba(test_set)[:, 1])\n",
    "    recall_of_positive = recall_score(y_test, predicted, zero_division=0)\n",
    "    return [model_name, ap_auc, f1_sc[1], recall_of_positive, acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1ea16080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to fit data on models and test the models on a test data set.\n",
    "def fit_and_evaluate_all(train_set, y_train, test_set, y_test, model_names, models):\n",
    "    \"\"\"\n",
    "    Function which trains a list of models on a train test and builds a dataframe containing a list of metrics \n",
    "    based on testing the models on the given test data.\n",
    "    :param train_set: The train dataset. It must be a pandas.Series containing rows of text.\n",
    "    :param y_train: The target values in the train set used for fitting the model.\n",
    "    :param test_set: The test dataset. It must be a pandas.Series containing rows of text.\n",
    "    :param y_test: The target values in the test set used for evaluating the reliability of the model.\n",
    "    :param model_names: A list of strings representing the names of the models to be trained and evaluated.\n",
    "    :param models: The a list of scikit-learn models that will be trained on the train data.\n",
    "    :return: a pd.Dataframe having scores defined at the function fit_and_evaluate for all the models defined at models\n",
    "    and their names defined at model_names.\n",
    "    \"\"\"\n",
    "    scores = pd.DataFrame(columns=[\"model_name\", \"average_precision\", \"f1\", \"recall\", \"accuracy\"])\n",
    "    for model, model_name in list(zip(models, model_names)):\n",
    "        row = fit_and_evaluate(train_set, y_train, test_set, y_test, model_name, model)\n",
    "        scores.loc[len(scores)] = row\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "60165933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>average_precision</th>\n",
       "      <th>f1</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>0.638826</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.822300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>0.533015</td>\n",
       "      <td>0.507692</td>\n",
       "      <td>0.891892</td>\n",
       "      <td>0.777003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SGD</td>\n",
       "      <td>0.541969</td>\n",
       "      <td>0.483221</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.731707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.538339</td>\n",
       "      <td>0.404494</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.630662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.580823</td>\n",
       "      <td>0.478873</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.742160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model_name  average_precision        f1    recall  accuracy\n",
       "0                Linear SVC           0.638826  0.585366  0.972973  0.822300\n",
       "0                Linear SVC           0.533015  0.507692  0.891892  0.777003\n",
       "1                       SGD           0.541969  0.483221  0.972973  0.731707\n",
       "2       Logistic Regression           0.538339  0.404494  0.972973  0.630662\n",
       "3  Random Forest Classifier           0.580823  0.478873  0.918919  0.742160"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit and predict. Then print statistics.\n",
    "final_results_with_mf = fit_and_evaluate_all(\n",
    "    X_SMOTEEN_with_mf, y_SMOTEEN_with_mf, X_test_with_mf, y_test, ['Linear SVC'], [svc])\n",
    "final_results_wo_mf = fit_and_evaluate_all(\n",
    "    X_SMOTEEN_wo_mf, y_SMOTEEN_wo_mf, X_test_wo_mf, y_test, \n",
    "    ['Linear SVC', 'SGD', 'Logistic Regression', 'Random Forest Classifier'], [svc, sgd, lr, rf])\n",
    "pd.concat([final_results_with_mf, final_results_wo_mf])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21c4394",
   "metadata": {},
   "source": [
    "It looks like we get the best performance when training a Linear Support Vector Machine on a dataset augmented by SMOTENN.\n",
    "Let's use gridsearch to see if we can obtain even better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6e62551f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.42 s\n",
      "Wall time: 53.2 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=SVC(class_weight=&#x27;balanced&#x27;, kernel=&#x27;linear&#x27;,\n",
       "                           probability=True, random_state=42),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{&#x27;C&#x27;: array([1.00000000e-04, 1.88739182e-04, 3.56224789e-04, 6.72335754e-04,\n",
       "       1.26896100e-03, 2.39502662e-03, 4.52035366e-03, 8.53167852e-03,\n",
       "       1.61026203e-02, 3.03919538e-02, 5.73615251e-02, 1.08263673e-01,\n",
       "       2.04335972e-01, 3.85662042e-01, 7.27895384e-01, 1.37382380e+00,\n",
       "       2.59294380e+00, 4.89390092e+00, 9.23670857e+00, 1.74332882e+01,\n",
       "       3.29034456e+01, 6.21016942e+01, 1.17210230e+02, 2.21221629e+02,\n",
       "       4.17531894e+02, 7.88046282e+02, 1.48735211e+03, 2.80721620e+03,\n",
       "       5.29831691e+03, 1.00000000e+04])}],\n",
       "             scoring=&#x27;average_precision&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=SVC(class_weight=&#x27;balanced&#x27;, kernel=&#x27;linear&#x27;,\n",
       "                           probability=True, random_state=42),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{&#x27;C&#x27;: array([1.00000000e-04, 1.88739182e-04, 3.56224789e-04, 6.72335754e-04,\n",
       "       1.26896100e-03, 2.39502662e-03, 4.52035366e-03, 8.53167852e-03,\n",
       "       1.61026203e-02, 3.03919538e-02, 5.73615251e-02, 1.08263673e-01,\n",
       "       2.04335972e-01, 3.85662042e-01, 7.27895384e-01, 1.37382380e+00,\n",
       "       2.59294380e+00, 4.89390092e+00, 9.23670857e+00, 1.74332882e+01,\n",
       "       3.29034456e+01, 6.21016942e+01, 1.17210230e+02, 2.21221629e+02,\n",
       "       4.17531894e+02, 7.88046282e+02, 1.48735211e+03, 2.80721620e+03,\n",
       "       5.29831691e+03, 1.00000000e+04])}],\n",
       "             scoring=&#x27;average_precision&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(class_weight=&#x27;balanced&#x27;, kernel=&#x27;linear&#x27;, probability=True, random_state=42)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(class_weight=&#x27;balanced&#x27;, kernel=&#x27;linear&#x27;, probability=True, random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=SVC(class_weight='balanced', kernel='linear',\n",
       "                           probability=True, random_state=42),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{'C': array([1.00000000e-04, 1.88739182e-04, 3.56224789e-04, 6.72335754e-04,\n",
       "       1.26896100e-03, 2.39502662e-03, 4.52035366e-03, 8.53167852e-03,\n",
       "       1.61026203e-02, 3.03919538e-02, 5.73615251e-02, 1.08263673e-01,\n",
       "       2.04335972e-01, 3.85662042e-01, 7.27895384e-01, 1.37382380e+00,\n",
       "       2.59294380e+00, 4.89390092e+00, 9.23670857e+00, 1.74332882e+01,\n",
       "       3.29034456e+01, 6.21016942e+01, 1.17210230e+02, 2.21221629e+02,\n",
       "       4.17531894e+02, 7.88046282e+02, 1.48735211e+03, 2.80721620e+03,\n",
       "       5.29831691e+03, 1.00000000e+04])}],\n",
       "             scoring='average_precision')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Define parameter grid for svc\n",
    "param_grid = [{'C': np.logspace(-4, 4, 30)}]\n",
    "\n",
    "# Define the grid search\n",
    "grid_search = GridSearchCV(SVC(random_state=42, class_weight='balanced', probability=True, kernel='linear'), param_grid=param_grid, \n",
    "                           scoring='average_precision', cv=StratifiedKFold(n_splits=5), n_jobs=-1)\n",
    "\n",
    "# Do the actual searching\n",
    "grid_search.fit(X_SMOTEEN_with_mf, y_SMOTEEN_with_mf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "90f918d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_C</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.373824</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.592944</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5298.316906</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2807.216204</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1487.352107</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        param_C  mean_test_score\n",
       "15     1.373824              1.0\n",
       "16     2.592944              1.0\n",
       "28  5298.316906              1.0\n",
       "27  2807.216204              1.0\n",
       "26  1487.352107              1.0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output the results\n",
    "pd.DataFrame(grid_search.cv_results_)[['param_C','mean_test_score']].sort_values(\n",
    "    by='mean_test_score', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4e8e999e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>average_precision</th>\n",
       "      <th>f1</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C=1.37</td>\n",
       "      <td>0.635812</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.8223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C=2.59</td>\n",
       "      <td>0.638826</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.8223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C=5298</td>\n",
       "      <td>0.638533</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.8223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C=2807</td>\n",
       "      <td>0.638533</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.8223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C=1487</td>\n",
       "      <td>0.638533</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.8223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model_name  average_precision        f1    recall  accuracy\n",
       "0     C=1.37           0.635812  0.585366  0.972973    0.8223\n",
       "1     C=2.59           0.638826  0.585366  0.972973    0.8223\n",
       "2     C=5298           0.638533  0.585366  0.972973    0.8223\n",
       "3     C=2807           0.638533  0.585366  0.972973    0.8223\n",
       "4     C=1487           0.638533  0.585366  0.972973    0.8223"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check top 5 results against test class\n",
    "\n",
    "fit_and_evaluate_all(X_SMOTEEN_with_mf, y_SMOTEEN_with_mf, X_test_with_mf, y_test,\n",
    "                     ['C=1.37', 'C=2.59', 'C=5298', 'C=2807', 'C=1487'], \n",
    "                     [SVC(random_state=42, class_weight='balanced', probability=True, kernel='linear', C=1.37),\n",
    "                     SVC(random_state=42, class_weight='balanced', probability=True, kernel='linear', C=2.59),\n",
    "                     SVC(random_state=42, class_weight='balanced', probability=True, kernel='linear', C=5298),\n",
    "                     SVC(random_state=42, class_weight='balanced', probability=True, kernel='linear', C=2807),\n",
    "                     SVC(random_state=42, class_weight='balanced', probability=True, kernel='linear', C=1487)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7388e4d",
   "metadata": {},
   "source": [
    "There is a slight improvement for the model having C=2.59, so we will pick that one.\n",
    "\n",
    "I will now save the model, the scaler and the vectorizer to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "34c02b1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=2.59, class_weight=&#x27;balanced&#x27;, kernel=&#x27;linear&#x27;, probability=True,\n",
       "    random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" checked><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=2.59, class_weight=&#x27;balanced&#x27;, kernel=&#x27;linear&#x27;, probability=True,\n",
       "    random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=2.59, class_weight='balanced', kernel='linear', probability=True,\n",
       "    random_state=42)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have the vectorizer already, but we need to create an SVC model object and fit it\n",
    "svc = SVC(random_state=42, class_weight='balanced', probability=True, kernel='linear', C=2.59)\n",
    "svc.fit(X_SMOTEEN_with_mf, y_SMOTEEN_with_mf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5bb9718e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tfidf-vectorizer to file\n",
    "with open(\"../data/modeling/tfidf_1.pkl\", \"wb\") as tfidf_file:\n",
    "    dump(tfidf_vectorizer_with_mf, tfidf_file)\n",
    "    tfidf_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f2a57f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tfidf-vectorizer to file\n",
    "with open(\"../data/modeling/scaler_1.pkl\", \"wb\") as scaler_file:\n",
    "    dump(scaler_with_mf, scaler_file)\n",
    "    scaler_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d9ce28cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to file\n",
    "with open(\"../data/modeling/model_1_with_mf.pkl\", \"wb\") as model_file:\n",
    "    dump(svc, model_file)\n",
    "    model_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c1498e",
   "metadata": {},
   "source": [
    "## Experiment\n",
    "\n",
    "What I would like to try is to artificially enhance my data set by translating the imbalanced data class into another language, then back to English.\n",
    "\n",
    "As we have seen above, we have 1249 negative class counts and 184 positive class counts. That means that the majority class is 6.8 times bigger. In order to counterbalance this, we need to generate 6 times the data that we have now for the minority class. Or, in simpler terms, we need to translate our messages in 6 different languages, then back to English.\n",
    "\n",
    "For this, I will create a function in a python script and call it in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "707db1de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "501    - __EMAIL__\n",
       " \n",
       "\n",
       " Ordered on June 20th, order nu...\n",
       "Name: text, dtype: string"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the indexes for all the positive examples in the training set\n",
    "positive_indexes = y_train.loc[y_train == 1].index.values\n",
    "\n",
    "# Get the actual psoitive examples in the training set\n",
    "X_train_pos = X_train[positive_indexes].astype('string')\n",
    "X_train_pos.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "425a034e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 13.8 s\n",
      "Wall time: 3min 58s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    - __EMAIL__\\n \\n\\n Ordered on June 20th, order...\n",
       "1    - __EMAIL__\\n \\n\\n Ordered on June 20, order n...\n",
       "2    - __E-MAIL__\\n \\n\\n Ordered on June 20, order ...\n",
       "3    - __E-MAIL__\\n \\n\\n Ordered June 20, order num...\n",
       "4    - __EMAIL__\\n \\n\\n Ordered June 20, order numb...\n",
       "dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Augment the data\n",
    "aug_X_train = ex.enhance_series(X_train_pos)\n",
    "aug_X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ec180084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "882"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create index to add the data correctly; Last index from the dataset is 1432, so we'll start at 1433 and end at 1443 + 882\n",
    "index_values = [x for x in range(1433, 1433+882)]\n",
    "len(index_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "73373618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1433    - __EMAIL__\\n \\n\\n Ordered on June 20th, order...\n",
       "1434    - __EMAIL__\\n \\n\\n Ordered on June 20, order n...\n",
       "1435    - __E-MAIL__\\n \\n\\n Ordered on June 20, order ...\n",
       "1436    - __E-MAIL__\\n \\n\\n Ordered June 20, order num...\n",
       "1437    - __EMAIL__\\n \\n\\n Ordered June 20, order numb...\n",
       "dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index the augmented text series\n",
    "aug_X_train.index = pd.Index(index_values)\n",
    "aug_X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e22039b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1433    1\n",
       "1434    1\n",
       "1435    1\n",
       "1436    1\n",
       "1437    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create indexed target series\n",
    "aug_y_train = pd.Series([1]*len(index_values), index=index_values)\n",
    "aug_y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a23bedd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2028,) (2028,)\n"
     ]
    }
   ],
   "source": [
    "# Now mix them all together to form a final training df\n",
    "final_X_train = pd.concat([X_train, aug_X_train])\n",
    "final_y_train = pd.concat([y_train, aug_y_train])\n",
    "print(final_X_train.shape, final_y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1cd5ad29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1029\n",
       "0     999\n",
       "dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check value counts to see if it's balanced\n",
    "final_y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7032fc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tfidf representation of train and test sets with manual features\n",
    "text_train, m_feats_train = get_text_m_feats(final_X_train, new_token_dict, True)\n",
    "\n",
    "tfidf_vectorizer_with_mf = get_tfidf_vectorizer(text_train)\n",
    "tfidf_train = tfidf_vectorizer_with_mf.transform(text_train)\n",
    "\n",
    "scaler_with_mf = get_scaler(m_feats_train)\n",
    "scaled_m_feats_train = pd.DataFrame(scaler_with_mf.transform(m_feats_train), index = m_feats_train.index.values)\n",
    "final_X_train_tfidf = hstack((tfidf_train, csr_matrix(scaled_m_feats_train)))\n",
    "\n",
    "final_X_test_tfidf = transform_to_tfidf_matrix(\n",
    "    new_token_dict, tfidf_vectorizer_with_mf, scaler_with_mf, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d66298c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>average_precision</th>\n",
       "      <th>f1</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.628295</td>\n",
       "      <td>0.653061</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.881533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>0.730447</td>\n",
       "      <td>0.738095</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.923345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.797544</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.937282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.706843</td>\n",
       "      <td>0.720930</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.916376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SGD Classifier</td>\n",
       "      <td>0.758444</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.926829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name  average_precision        f1    recall  accuracy\n",
       "0             Naive Bayes           0.628295  0.653061  0.864865  0.881533\n",
       "1              Linear SVC           0.730447  0.738095  0.837838  0.923345\n",
       "2  RandomForestClassifier           0.797544  0.780488  0.864865  0.937282\n",
       "3     Logistic Regression           0.706843  0.720930  0.837838  0.916376\n",
       "4          SGD Classifier           0.758444  0.740741  0.810811  0.926829"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = fit_and_evaluate_all(final_X_train_tfidf, final_y_train, final_X_test_tfidf, y_test, model_names, models)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c500339e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tfidf representation of train and test sets without manual features\n",
    "text_train, _ = get_text_m_feats(final_X_train, new_token_dict, False)\n",
    "\n",
    "tfidf_vectorizer_wo_mf = get_tfidf_vectorizer(text_train)\n",
    "final_X_train_tfidf = tfidf_vectorizer_wo_mf.transform(text_train)\n",
    "\n",
    "final_X_test_tfidf = transform_to_tfidf_matrix(new_token_dict, tfidf_vectorizer_wo_mf, None, X_test, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "10c95597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>average_precision</th>\n",
       "      <th>f1</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.656500</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.874564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>0.698138</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.923345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.790456</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.891892</td>\n",
       "      <td>0.926829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.674859</td>\n",
       "      <td>0.712644</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.912892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SGD Classifier</td>\n",
       "      <td>0.730498</td>\n",
       "      <td>0.708861</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.919861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name  average_precision        f1    recall  accuracy\n",
       "0             Naive Bayes           0.656500  0.625000  0.810811  0.874564\n",
       "1              Linear SVC           0.698138  0.731707  0.810811  0.923345\n",
       "2  RandomForestClassifier           0.790456  0.758621  0.891892  0.926829\n",
       "3     Logistic Regression           0.674859  0.712644  0.837838  0.912892\n",
       "4          SGD Classifier           0.730498  0.708861  0.756757  0.919861"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = fit_and_evaluate_all(final_X_train_tfidf, final_y_train, final_X_test_tfidf, y_test, model_names, models)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63200a8c",
   "metadata": {},
   "source": [
    "It looks like we got a much better result using this type of data augmentation. Here we have obtained the best result by far with a RandomForestClassifier using the manual features as well.\n",
    "\n",
    "I will see if hypertuning it will improve this result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "da096f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset with manual features\n",
    "text_train, m_feats_train = get_text_m_feats(final_X_train, new_token_dict, True)\n",
    "\n",
    "tfidf_vectorizer_with_mf = get_tfidf_vectorizer(text_train)\n",
    "tfidf_train = tfidf_vectorizer_with_mf.transform(text_train)\n",
    "\n",
    "scaler_with_mf = get_scaler(m_feats_train)\n",
    "scaled_m_feats_train = pd.DataFrame(scaler_with_mf.transform(m_feats_train), index = m_feats_train.index.values)\n",
    "final_X_train_tfidf = hstack((tfidf_train, csr_matrix(scaled_m_feats_train)))\n",
    "\n",
    "final_X_test_tfidf = transform_to_tfidf_matrix(\n",
    "    new_token_dict, tfidf_vectorizer_with_mf, scaler_with_mf, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "01bce047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3.98 s\n",
      "Wall time: 1min 51s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_criterion</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200</td>\n",
       "      <td>gini</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>0.981887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>gini</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>0.981221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>200</td>\n",
       "      <td>entropy</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>0.979862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>200</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>0.979862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>200</td>\n",
       "      <td>gini</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>0.979398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>gini</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>0.978286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>100</td>\n",
       "      <td>entropy</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>0.978226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   param_n_estimators param_criterion param_max_depth param_min_samples_split  \\\n",
       "2                 200            gini            None                       2   \n",
       "1                 100            gini            None                       2   \n",
       "29                200         entropy            None                       2   \n",
       "56                200        log_loss            None                       2   \n",
       "5                 200            gini            None                       4   \n",
       "4                 100            gini            None                       4   \n",
       "28                100         entropy            None                       2   \n",
       "\n",
       "    mean_test_score  \n",
       "2          0.981887  \n",
       "1          0.981221  \n",
       "29         0.979862  \n",
       "56         0.979862  \n",
       "5          0.979398  \n",
       "4          0.978286  \n",
       "28         0.978226  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Define parameter grid for svc\n",
    "param_grid = [{'n_estimators' : [50, 100, 200],\n",
    "              'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "              'max_depth': [None, 2, 6],\n",
    "              'min_samples_split': [2, 4, 10]}]\n",
    "\n",
    "# Define the grid search\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=42, n_jobs=-1), param_grid=param_grid, \n",
    "                           scoring='average_precision', cv=StratifiedKFold(n_splits=5), n_jobs=-1)\n",
    "\n",
    "# Do the actual searching\n",
    "grid_search.fit(final_X_train_tfidf, final_y_train)\n",
    "\n",
    "# Output the results\n",
    "pd.DataFrame(grid_search.cv_results_)[['param_n_estimators', 'param_criterion', 'param_max_depth', \n",
    "                                       'param_min_samples_split','mean_test_score']].sort_values(\n",
    "    by='mean_test_score', ascending=False).head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4a4829c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>average_precision</th>\n",
       "      <th>f1</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gini-200</td>\n",
       "      <td>0.801690</td>\n",
       "      <td>0.771084</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.933798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>entropy-200</td>\n",
       "      <td>0.806606</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.937282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>log_loss-200</td>\n",
       "      <td>0.806606</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.937282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gini-100-4</td>\n",
       "      <td>0.802272</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.891892</td>\n",
       "      <td>0.937282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gini-100</td>\n",
       "      <td>0.808916</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.926829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     model_name  average_precision        f1    recall  accuracy\n",
       "0      gini-200           0.801690  0.771084  0.864865  0.933798\n",
       "1   entropy-200           0.806606  0.780488  0.864865  0.937282\n",
       "2  log_loss-200           0.806606  0.780488  0.864865  0.937282\n",
       "3    gini-100-4           0.802272  0.785714  0.891892  0.937282\n",
       "4      gini-100           0.808916  0.746988  0.837838  0.926829"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check top 5 results against test class\n",
    "\n",
    "fit_and_evaluate_all(final_X_train_tfidf, final_y_train, final_X_test_tfidf, y_test,\n",
    "                     ['gini-200', 'entropy-200', 'log_loss-200', 'gini-100-4', 'gini-100'], \n",
    "                     [RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1, criterion='gini'),\n",
    "                     RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1, criterion='entropy'),\n",
    "                     RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1, criterion='log_loss'),\n",
    "                     RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, criterion='gini'),\n",
    "                     RandomForestClassifier(n_estimators=200, min_samples_split=4, random_state=42, n_jobs=-1, criterion='gini')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7a1e70",
   "metadata": {},
   "source": [
    "We can see that hypertuning improved the average_precision accuracy from 0.797 to 0.802 and the recall score from 0.864 to 0.891. We will store the scaler, vectorizers and model having the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9a744707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(criterion=&#x27;entropy&#x27;, n_jobs=-1, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(criterion=&#x27;entropy&#x27;, n_jobs=-1, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', n_jobs=-1, random_state=42)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have the vectorizer already, but we need to create an RandomForestClassifier model object and fit it\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, criterion='entropy')\n",
    "rf.fit(final_X_train_tfidf, final_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e41d9c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tfidf-vectorizer to file\n",
    "with open(\"../data/modeling/tfidf_2.pkl\", \"wb\") as tfidf_file:\n",
    "    dump(tfidf_vectorizer_with_mf, tfidf_file)\n",
    "    tfidf_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2dd7b2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tfidf-vectorizer to file\n",
    "with open(\"../data/modeling/scaler_2.pkl\", \"wb\") as scaler_file:\n",
    "    dump(scaler_with_mf, scaler_file)\n",
    "    scaler_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0f6aa541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to file\n",
    "with open(\"../data/modeling/model_2_with_mf.pkl\", \"wb\") as model_file:\n",
    "    dump(rf, model_file)\n",
    "    model_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
